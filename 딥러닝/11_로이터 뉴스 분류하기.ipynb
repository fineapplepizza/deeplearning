{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c351bc",
   "metadata": {},
   "source": [
    "문장의 특성을 이해하면서 분석하게 된다\n",
    "### 로이터 뉴스 데이터 로딩 및 탐색 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d689849",
   "metadata": {},
   "source": [
    "#### 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be562225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a534efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SM2127\\anaconda3\\envs\\deep01\\lib\\site-packages\\tensorflow_core\\python\\keras\\datasets\\reuters.py:113: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\SM2127\\anaconda3\\envs\\deep01\\lib\\site-packages\\tensorflow_core\\python\\keras\\datasets\\reuters.py:114: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(X_train,y_train), (X_test,y_test) = reuters.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b13189a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8982,), (8982,), (2246,), (2246,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c773ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정답 종류 확인\n",
    "# 0 ~ 45 : 46개로 분류된 뉴스 데이터 \n",
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4b83b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     3159\n",
       "4     1949\n",
       "19     549\n",
       "16     444\n",
       "1      432\n",
       "11     390\n",
       "20     269\n",
       "13     172\n",
       "8      139\n",
       "10     124\n",
       "9      101\n",
       "21     100\n",
       "25      92\n",
       "2       74\n",
       "18      66\n",
       "24      62\n",
       "0       55\n",
       "34      50\n",
       "36      49\n",
       "12      49\n",
       "28      48\n",
       "6       48\n",
       "30      45\n",
       "23      41\n",
       "31      39\n",
       "17      39\n",
       "40      36\n",
       "32      32\n",
       "41      30\n",
       "14      26\n",
       "39      24\n",
       "26      24\n",
       "43      21\n",
       "15      20\n",
       "29      19\n",
       "37      19\n",
       "38      19\n",
       "45      18\n",
       "5       17\n",
       "7       16\n",
       "27      15\n",
       "22      15\n",
       "42      13\n",
       "44      12\n",
       "33      11\n",
       "35      10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정답 종류별 갯수 확인\n",
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53618b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 27595,\n",
       " 28842,\n",
       " 8,\n",
       " 43,\n",
       " 10,\n",
       " 447,\n",
       " 5,\n",
       " 25,\n",
       " 207,\n",
       " 270,\n",
       " 5,\n",
       " 3095,\n",
       " 111,\n",
       " 16,\n",
       " 369,\n",
       " 186,\n",
       " 90,\n",
       " 67,\n",
       " 7,\n",
       " 89,\n",
       " 5,\n",
       " 19,\n",
       " 102,\n",
       " 6,\n",
       " 19,\n",
       " 124,\n",
       " 15,\n",
       " 90,\n",
       " 67,\n",
       " 84,\n",
       " 22,\n",
       " 482,\n",
       " 26,\n",
       " 7,\n",
       " 48,\n",
       " 4,\n",
       " 49,\n",
       " 8,\n",
       " 864,\n",
       " 39,\n",
       " 209,\n",
       " 154,\n",
       " 6,\n",
       " 151,\n",
       " 6,\n",
       " 83,\n",
       " 11,\n",
       " 15,\n",
       " 22,\n",
       " 155,\n",
       " 11,\n",
       " 15,\n",
       " 7,\n",
       " 48,\n",
       " 9,\n",
       " 4579,\n",
       " 1005,\n",
       " 504,\n",
       " 6,\n",
       " 258,\n",
       " 6,\n",
       " 272,\n",
       " 11,\n",
       " 15,\n",
       " 22,\n",
       " 134,\n",
       " 44,\n",
       " 11,\n",
       " 15,\n",
       " 16,\n",
       " 8,\n",
       " 197,\n",
       " 1245,\n",
       " 90,\n",
       " 67,\n",
       " 52,\n",
       " 29,\n",
       " 209,\n",
       " 30,\n",
       " 32,\n",
       " 132,\n",
       " 6,\n",
       " 109,\n",
       " 15,\n",
       " 17,\n",
       " 12]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0] #뉴스데이터가 토큰화,수치화 진행된 데이터 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d068182b",
   "metadata": {},
   "source": [
    "- 뉴스 기사를 토큰화, 수치화를 진행한 데이터셋\n",
    "- 뉴스 기사마다 길이가 다르다 \n",
    "- 수치화를 진행할 때 단어의 빈도수를 기반으로 수치화 진행\n",
    "- 숫자가 작을수록 자주 등장한 단어 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cd36c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mdbl': 10996,\n",
       " 'fawc': 16260,\n",
       " 'degussa': 12089,\n",
       " 'woods': 8803,\n",
       " 'hanging': 13796,\n",
       " 'localized': 20672,\n",
       " 'sation': 20673,\n",
       " 'chanthaburi': 20675,\n",
       " 'refunding': 10997,\n",
       " 'hermann': 8804,\n",
       " 'passsengers': 20676,\n",
       " 'stipulate': 20677,\n",
       " 'heublein': 8352,\n",
       " 'screaming': 20713,\n",
       " 'tcby': 16261,\n",
       " 'four': 185,\n",
       " 'grains': 1642,\n",
       " 'broiler': 20680,\n",
       " 'wooden': 12090,\n",
       " 'wednesday': 1220,\n",
       " 'highveld': 13797,\n",
       " 'duffour': 7593,\n",
       " '0053': 20681,\n",
       " 'elections': 3914,\n",
       " '270': 2563,\n",
       " '271': 3551,\n",
       " '272': 5113,\n",
       " '273': 3552,\n",
       " '274': 3400,\n",
       " 'rudman': 7975,\n",
       " '276': 3401,\n",
       " '277': 3478,\n",
       " '278': 3632,\n",
       " '279': 4309,\n",
       " 'dormancy': 9381,\n",
       " 'errors': 7247,\n",
       " 'deferred': 3086,\n",
       " 'sptnd': 20683,\n",
       " 'cooking': 8805,\n",
       " 'stratabit': 20684,\n",
       " 'designing': 16262,\n",
       " 'metalurgicos': 20685,\n",
       " 'databank': 13798,\n",
       " '300er': 20686,\n",
       " 'shocks': 20687,\n",
       " 'nawg': 7972,\n",
       " 'tnta': 20688,\n",
       " 'perforations': 20689,\n",
       " 'affiliates': 2891,\n",
       " '27p': 20690,\n",
       " 'ching': 16263,\n",
       " 'china': 595,\n",
       " 'wagyu': 16264,\n",
       " 'affiliated': 3189,\n",
       " 'chino': 16265,\n",
       " 'chinh': 16266,\n",
       " 'slickline': 20692,\n",
       " 'doldrums': 13799,\n",
       " 'kids': 12092,\n",
       " 'climbed': 3028,\n",
       " 'controversy': 6693,\n",
       " 'kidd': 20693,\n",
       " 'spotty': 12093,\n",
       " 'rebel': 12639,\n",
       " 'millimetres': 9382,\n",
       " 'golden': 4007,\n",
       " 'projection': 5689,\n",
       " 'stern': 12094,\n",
       " \"hudson's\": 7903,\n",
       " 'dna': 10066,\n",
       " 'dnc': 20695,\n",
       " 'hodler': 20696,\n",
       " 'lme': 2394,\n",
       " 'insolvancy': 20697,\n",
       " 'music': 13800,\n",
       " 'therefore': 1984,\n",
       " 'dns': 10998,\n",
       " 'distortions': 6959,\n",
       " 'thassos': 13801,\n",
       " 'populations': 20698,\n",
       " 'meteorologist': 8806,\n",
       " 'loss': 43,\n",
       " 'exco': 9383,\n",
       " 'adventist': 20813,\n",
       " 'murchison': 16267,\n",
       " 'locked': 10999,\n",
       " 'kampala': 13802,\n",
       " 'arndt': 20699,\n",
       " 'nakasone': 1267,\n",
       " 'steinweg': 20700,\n",
       " \"india's\": 3633,\n",
       " 'wang': 3029,\n",
       " 'wane': 10067,\n",
       " 'unjust': 13803,\n",
       " 'titanium': 13804,\n",
       " 'want': 850,\n",
       " 'pinto': 20701,\n",
       " \"institutes'\": 16268,\n",
       " 'absolute': 7973,\n",
       " 'travel': 4677,\n",
       " 'cutback': 6422,\n",
       " 'nazmi': 16269,\n",
       " 'modest': 1858,\n",
       " 'shopwell': 16270,\n",
       " 'sedi': 20702,\n",
       " 'adoped': 20703,\n",
       " 'tulis': 16271,\n",
       " '18th': 20704,\n",
       " \"wmc's\": 20705,\n",
       " 'menlo': 20706,\n",
       " 'reiners': 11000,\n",
       " 'farmlands': 12095,\n",
       " 'nonsensical': 20707,\n",
       " 'elisra': 20708,\n",
       " 'welcomed': 2461,\n",
       " 'peup': 20709,\n",
       " \"holiday's\": 16272,\n",
       " 'activating': 20711,\n",
       " 'avondale': 16273,\n",
       " 'interational': 16274,\n",
       " 'welcomes': 20712,\n",
       " 'fip': 16275,\n",
       " 'tailings': 11001,\n",
       " 'fit': 4205,\n",
       " 'lifeline': 16276,\n",
       " 'bringing': 1916,\n",
       " 'fix': 4819,\n",
       " '624': 6164,\n",
       " 'naturalite': 12096,\n",
       " 'wales': 6165,\n",
       " 'fin': 8807,\n",
       " 'fio': 11129,\n",
       " 'ceremenony': 20714,\n",
       " 'sovr': 20715,\n",
       " \"yeo's\": 20716,\n",
       " 'effects': 1788,\n",
       " 'sixteen': 13805,\n",
       " 'undeveloped': 8808,\n",
       " 'glutted': 13806,\n",
       " 'barton': 20717,\n",
       " 'froday': 20718,\n",
       " 'arrow': 10089,\n",
       " 'stabilises': 11002,\n",
       " 'allan': 6960,\n",
       " '374p': 20719,\n",
       " '393': 3891,\n",
       " '392': 4008,\n",
       " '391': 4206,\n",
       " '390': 3079,\n",
       " '397': 4550,\n",
       " '396': 6166,\n",
       " '395': 6423,\n",
       " '394': 4207,\n",
       " '399': 6961,\n",
       " '398': 4208,\n",
       " 'stabilised': 7595,\n",
       " 'smelters': 5114,\n",
       " 'oprah': 20720,\n",
       " 'orginially': 20721,\n",
       " \"tvx's\": 20722,\n",
       " 'ponomarev': 16278,\n",
       " 'enviroment': 20723,\n",
       " \"reeves'\": 20724,\n",
       " 'mason': 8363,\n",
       " 'encourage': 1670,\n",
       " 'adapt': 7596,\n",
       " 'abbott': 12776,\n",
       " 'stamping': 13808,\n",
       " 'colquiri': 20726,\n",
       " 'ambrit': 11003,\n",
       " 'strata': 8353,\n",
       " 'corrects': 4821,\n",
       " 'sandra': 11922,\n",
       " 'estimate': 859,\n",
       " 'universally': 20727,\n",
       " 'chlorine': 20728,\n",
       " 'competes': 16279,\n",
       " 'leiner': 10068,\n",
       " 'ministries': 8809,\n",
       " 'disturbed': 8810,\n",
       " 'competed': 13809,\n",
       " 'juergen': 8811,\n",
       " 'kfw': 13810,\n",
       " 'turben': 11004,\n",
       " 'reintroduced': 9384,\n",
       " 'maladies': 20729,\n",
       " 'chevron': 4101,\n",
       " 'lazere': 16280,\n",
       " 'antilles': 8812,\n",
       " 'dti': 11907,\n",
       " 'specially': 9070,\n",
       " 'bilzerian': 4678,\n",
       " 'bakelite': 13811,\n",
       " 'renovated': 20730,\n",
       " 'service': 568,\n",
       " 'payless': 16281,\n",
       " 'spiegler': 20731,\n",
       " 'needed': 831,\n",
       " 'wigglesworth': 16282,\n",
       " 'master': 6962,\n",
       " 'antonson': 13812,\n",
       " 'genesis': 20732,\n",
       " 'vismara': 13813,\n",
       " 'organically': 20734,\n",
       " \"accords'\": 20735,\n",
       " 'task': 5940,\n",
       " 'positively': 7974,\n",
       " 'feasibility': 3479,\n",
       " 'ahmed': 6963,\n",
       " \"suralco's\": 13814,\n",
       " 'awacs': 20736,\n",
       " 'idly': 16283,\n",
       " 'regulator': 20737,\n",
       " 'pseudorabies': 12097,\n",
       " 'staubli': 16284,\n",
       " 'nzi': 8813,\n",
       " 'feeling': 5115,\n",
       " '275': 3127,\n",
       " '6819': 20738,\n",
       " 'gorman': 16285,\n",
       " 'sustaining': 8354,\n",
       " 'spectrum': 9385,\n",
       " 'consenting': 20739,\n",
       " 'recapitalized': 12098,\n",
       " 'sailed': 11562,\n",
       " 'dozen': 7597,\n",
       " 'affairs': 1985,\n",
       " 'courier': 2253,\n",
       " 'kremlin': 8355,\n",
       " 'shipments': 895,\n",
       " \"aquino's\": 16286,\n",
       " 'committing': 10070,\n",
       " 'sugarcane': 5293,\n",
       " 'diminishing': 9386,\n",
       " 'vexing': 16287,\n",
       " 'simplify': 11005,\n",
       " 'mouth': 6167,\n",
       " 'steinhardt': 7248,\n",
       " 'conceded': 8814,\n",
       " 'bradford': 9387,\n",
       " 'singer': 7976,\n",
       " '5602': 20740,\n",
       " \"1987's\": 13816,\n",
       " 'tech': 4950,\n",
       " 'teck': 6424,\n",
       " 'majv': 20741,\n",
       " 'saying': 666,\n",
       " 'dickey': 16477,\n",
       " 'sweetner': 20742,\n",
       " 'teresa': 21149,\n",
       " 'ulcer': 20743,\n",
       " 'cheaply': 13817,\n",
       " 'thai': 2361,\n",
       " 'orleans': 6964,\n",
       " 'excavator': 16290,\n",
       " 'rico': 6168,\n",
       " 'lube': 12099,\n",
       " 'rick': 13818,\n",
       " 'rich': 4679,\n",
       " 'kerna': 13819,\n",
       " 'rice': 950,\n",
       " 'rica': 4209,\n",
       " 'plate': 5503,\n",
       " 'platt': 16291,\n",
       " 'altogether': 8356,\n",
       " 'jaguar': 8815,\n",
       " 'dynair': 20744,\n",
       " 'patch': 8816,\n",
       " 'ldp': 2892,\n",
       " 'boarded': 13820,\n",
       " 'precluding': 16292,\n",
       " 'clarified': 11006,\n",
       " 'sensitivity': 16293,\n",
       " 'alternative': 1511,\n",
       " 'clarifies': 11007,\n",
       " 'lots': 5116,\n",
       " 'irs': 7598,\n",
       " 'irv': 20745,\n",
       " 'iri': 13821,\n",
       " 'ira': 13822,\n",
       " 'timber': 5690,\n",
       " 'ire': 20746,\n",
       " 'discipline': 5219,\n",
       " 'extend': 1937,\n",
       " 'nature': 3634,\n",
       " \"amb's\": 16295,\n",
       " 'dunhill': 16296,\n",
       " 'extent': 2142,\n",
       " 'restrcitions': 20747,\n",
       " 'heating': 2396,\n",
       " \"mannesmann's\": 11008,\n",
       " 'outsanding': 20748,\n",
       " 'multimillions': 20749,\n",
       " 'sarcinelli': 13824,\n",
       " 'southeastern': 6694,\n",
       " 'eradicate': 10071,\n",
       " 'libyan': 9388,\n",
       " 'foreclosing': 20750,\n",
       " 'maclaine': 12101,\n",
       " 'fra': 20751,\n",
       " 'union': 353,\n",
       " 'frn': 11009,\n",
       " 'much': 386,\n",
       " 'fry': 12102,\n",
       " 'mothball': 20752,\n",
       " 'chlorazepate': 10072,\n",
       " 'dxns': 12103,\n",
       " 'toyko': 19981,\n",
       " 'spit': 20753,\n",
       " '007050': 16297,\n",
       " 'freehold': 16298,\n",
       " 'davy': 13825,\n",
       " 'dave': 11010,\n",
       " 'spie': 12177,\n",
       " 'aguayo': 10117,\n",
       " 'wildcat': 12104,\n",
       " 'fecs': 10069,\n",
       " 'kennan': 20754,\n",
       " 'intal': 16299,\n",
       " 'contingencies': 9389,\n",
       " 'professionally': 16551,\n",
       " 'microbiological': 16300,\n",
       " 'misconstrued': 20756,\n",
       " 'k': 409,\n",
       " 'securitiesd': 20757,\n",
       " 'deferring': 16301,\n",
       " 'kohl': 5941,\n",
       " 'conditioned': 3030,\n",
       " 'fnhb': 20758,\n",
       " \"october's\": 16302,\n",
       " 'memorial': 13954,\n",
       " 'democracies': 6965,\n",
       " 'conformed': 27520,\n",
       " 'split': 464,\n",
       " \"bond's\": 12105,\n",
       " 'thinly': 11112,\n",
       " 'dunkirk': 16515,\n",
       " 'cavanaugh': 16303,\n",
       " \"securities'\": 13827,\n",
       " 'marches': 21345,\n",
       " 'issam': 16304,\n",
       " 'workforce': 2020,\n",
       " 'meinert': 12106,\n",
       " 'boiler': 13828,\n",
       " \"bp's\": 5294,\n",
       " 'torpedoed': 16305,\n",
       " 'indidate': 20762,\n",
       " 'downwardly': 13829,\n",
       " 'viviez': 20763,\n",
       " 'vladiminovich': 20764,\n",
       " 'academic': 16306,\n",
       " 'architecural': 20765,\n",
       " 'corporate': 1117,\n",
       " 'appropriately': 16307,\n",
       " 'teicc': 20766,\n",
       " \"hanover's\": 20767,\n",
       " 'aristech': 8817,\n",
       " 'portrayed': 20768,\n",
       " 'raffineries': 21383,\n",
       " 'hai': 20770,\n",
       " 'hal': 7599,\n",
       " 'ham': 13830,\n",
       " 'han': 10073,\n",
       " 'e15b': 20771,\n",
       " 'had': 61,\n",
       " 'hay': 20772,\n",
       " 'botchwey': 13831,\n",
       " 'haq': 10074,\n",
       " 'has': 37,\n",
       " 'hat': 13832,\n",
       " 'hav': 20773,\n",
       " 'fortin': 20774,\n",
       " 'municipal': 8818,\n",
       " 'osman': 20775,\n",
       " 'fsical': 20776,\n",
       " 'elders': 3480,\n",
       " 'survival': 12107,\n",
       " 'unequivocally': 16308,\n",
       " 'objective': 2519,\n",
       " 'indicative': 6695,\n",
       " 'shadow': 10075,\n",
       " 'riskiness': 21411,\n",
       " 'positiive': 20778,\n",
       " \"american's\": 10076,\n",
       " 'alick': 16309,\n",
       " 'harima': 16310,\n",
       " 'alice': 12108,\n",
       " 'altschul': 20779,\n",
       " 'festivities': 16311,\n",
       " 'medecines': 20780,\n",
       " 'beneficial': 2942,\n",
       " 'yoweri': 12109,\n",
       " 'crowd': 13833,\n",
       " 'crowe': 9390,\n",
       " 'crown': 3553,\n",
       " 'topping': 13679,\n",
       " 'captive': 8819,\n",
       " 'billboard': 12110,\n",
       " 'fiduciary': 6169,\n",
       " 'bottom': 3402,\n",
       " 'plucked': 20782,\n",
       " 'locksmithing': 20783,\n",
       " 'ecopetrol': 9391,\n",
       " 'pipestone': 24018,\n",
       " \"growers'\": 5505,\n",
       " 'borrows': 20785,\n",
       " 'eduard': 16312,\n",
       " 'venpres': 13834,\n",
       " 'bamboo': 16313,\n",
       " 'foolish': 13835,\n",
       " 'uruguyan': 20786,\n",
       " 'officeholders': 20787,\n",
       " 'economiques': 20788,\n",
       " 'aden': 16314,\n",
       " 'maxwell': 4822,\n",
       " 'marshall': 4680,\n",
       " 'honeymoon': 16315,\n",
       " 'administer': 16316,\n",
       " 'shoots': 20790,\n",
       " 'rubbertech': 16317,\n",
       " 'johsen': 16318,\n",
       " 'reciprocity': 10077,\n",
       " 'fabric': 13836,\n",
       " 'suffice': 20791,\n",
       " 'spokemsan': 20792,\n",
       " \"sonora's\": 20793,\n",
       " '5865': 16319,\n",
       " \"systems'\": 16320,\n",
       " 'perfumes': 20794,\n",
       " 'halycon': 20795,\n",
       " 'nonvoting': 20796,\n",
       " 'safeguard': 7250,\n",
       " 'sawdust': 21538,\n",
       " \"else's\": 20797,\n",
       " 'arrays': 13837,\n",
       " 'aza': 20798,\n",
       " 'smasher': 20799,\n",
       " 'complications': 12111,\n",
       " 'pesos': 1813,\n",
       " 'relabelling': 20800,\n",
       " 'passenger': 3722,\n",
       " \"avon's\": 12112,\n",
       " 'megahertz': 20801,\n",
       " 'mirror': 10683,\n",
       " 'minas': 8357,\n",
       " 'bourdain': 16322,\n",
       " 'crownx': 20802,\n",
       " 'eventual': 6425,\n",
       " 'crowns': 1207,\n",
       " 'role': 1369,\n",
       " 'obliges': 20803,\n",
       " 'rolf': 16323,\n",
       " 'vegetative': 13838,\n",
       " 'rolm': 20804,\n",
       " 'roll': 4419,\n",
       " 'intend': 2463,\n",
       " 'palms': 16324,\n",
       " 'denys': 19255,\n",
       " 'transported': 13839,\n",
       " 'moresby': 20805,\n",
       " 'devon': 16325,\n",
       " 'intent': 1351,\n",
       " \"camco's\": 20806,\n",
       " 'variable': 5942,\n",
       " 'transporter': 20807,\n",
       " 'danske': 16326,\n",
       " 'friedhelm': 13840,\n",
       " 'hawker': 8358,\n",
       " \"sand's\": 17774,\n",
       " 'preseving': 20808,\n",
       " '80386': 12113,\n",
       " 'bnls': 16328,\n",
       " 'ordination': 19984,\n",
       " 'overturned': 11011,\n",
       " 'erred': 16329,\n",
       " 'cincinnati': 6696,\n",
       " 'corps': 16710,\n",
       " 'whoever': 20809,\n",
       " 'osp': 16330,\n",
       " 'osr': 13841,\n",
       " 'ost': 12114,\n",
       " 'chair': 16331,\n",
       " '690': 5647,\n",
       " 'grapples': 20810,\n",
       " 'megawatts': 13842,\n",
       " 'photocopiers': 20811,\n",
       " 'sconninx': 20812,\n",
       " 'circumstances': 2274,\n",
       " 'oversight': 13843,\n",
       " \"paradyne's\": 20814,\n",
       " '691': 6363,\n",
       " 'paychecks': 20815,\n",
       " \"stadelmann's\": 13844,\n",
       " 'choice': 3241,\n",
       " 'vastagh': 11012,\n",
       " 'embark': 8820,\n",
       " 'gloomy': 9392,\n",
       " 'stays': 9393,\n",
       " 'exact': 4009,\n",
       " 'minute': 5117,\n",
       " 'kittiwake': 11892,\n",
       " 'picul': 20816,\n",
       " 'skewed': 20817,\n",
       " 'cooke': 11013,\n",
       " 'defaults': 10078,\n",
       " 'reimpose': 11014,\n",
       " 'hindered': 9394,\n",
       " 'lengthened': 20818,\n",
       " 'chopping': 16333,\n",
       " 'mckiernan': 13845,\n",
       " 'collaspe': 20819,\n",
       " 'corazon': 7251,\n",
       " 'antwerp': 7600,\n",
       " 'abdullah': 13846,\n",
       " 'goldston': 13847,\n",
       " '300': 442,\n",
       " 'cassa': 20821,\n",
       " 'casse': 20822,\n",
       " '695': 4081,\n",
       " 'ground': 2979,\n",
       " 'boost': 839,\n",
       " 'azusa': 16334,\n",
       " 'drafted': 9395,\n",
       " '303': 4823,\n",
       " 'climbs': 13848,\n",
       " 'honour': 7601,\n",
       " 'vanderbilt': 20823,\n",
       " '305': 3968,\n",
       " 'address': 3031,\n",
       " 'dwindling': 8821,\n",
       " 'benson': 7252,\n",
       " 'enroll': 12115,\n",
       " 'revenues': 501,\n",
       " 'impacted': 12116,\n",
       " 'queue': 20826,\n",
       " 'accomplished': 10079,\n",
       " 'throughput': 7602,\n",
       " 'influx': 9396,\n",
       " 'stockbuilding': 10080,\n",
       " 'aproximates': 20827,\n",
       " 'petroleo': 13849,\n",
       " 'sistemas': 16335,\n",
       " 'feretti': 14053,\n",
       " 'opposes': 5943,\n",
       " 'working': 882,\n",
       " 'perished': 20829,\n",
       " 'oldham': 13850,\n",
       " '27000': 20830,\n",
       " 'optimize': 19245,\n",
       " 'vigour': 20832,\n",
       " 'opposed': 1580,\n",
       " 'liberalizing': 16336,\n",
       " 'wvz': 20833,\n",
       " 'dampness': 20834,\n",
       " 'approving': 13851,\n",
       " 'sierra': 13496,\n",
       " 'entrepot': 20835,\n",
       " 'currency': 224,\n",
       " 'originally': 1499,\n",
       " 'tindemans': 20837,\n",
       " 'valorem': 16337,\n",
       " 'following': 477,\n",
       " 'fossen': 20838,\n",
       " 'locke': 11016,\n",
       " 'employess': 20839,\n",
       " 'rotberg': 12117,\n",
       " 'parachute': 16338,\n",
       " 'locks': 11017,\n",
       " 'incremental': 12255,\n",
       " 'woolowrth': 16339,\n",
       " 'listens': 20841,\n",
       " 'litre': 7253,\n",
       " 'edouard': 3554,\n",
       " 'ounce': 1377,\n",
       " 'nicanor': 20843,\n",
       " 'sucocitrico': 20844,\n",
       " 'minicomputers': 16340,\n",
       " \"silva's\": 16341,\n",
       " 'restitutions': 11018,\n",
       " 'custer': 16342,\n",
       " '3rd': 2590,\n",
       " 'fueled': 10081,\n",
       " 'trydahl': 20845,\n",
       " 'aice': 11019,\n",
       " 'harmon': 12118,\n",
       " 'conscious': 10082,\n",
       " 'herbicidesand': 20846,\n",
       " 'subdivisions': 20847,\n",
       " \"veslefrikk's\": 20848,\n",
       " 'swollen': 11020,\n",
       " 'pulled': 7978,\n",
       " 'tilney': 20849,\n",
       " 'years': 203,\n",
       " 'structuring': 20850,\n",
       " 'episodes': 20851,\n",
       " 'sportscene': 16343,\n",
       " \"northair's\": 16344,\n",
       " 'jig': 20852,\n",
       " 'jin': 20853,\n",
       " 'jim': 3403,\n",
       " 'troubles': 8359,\n",
       " 'workforces': 13852,\n",
       " 'suspension': 2362,\n",
       " 'troubled': 3892,\n",
       " 'fondiaria': 16345,\n",
       " 'modestly': 6697,\n",
       " 'recipients': 12119,\n",
       " 'civilian': 7979,\n",
       " 'indigenous': 13853,\n",
       " 'overpowering': 20854,\n",
       " 'drilling': 1051,\n",
       " 'sorted': 16346,\n",
       " 'lichtenstein': 16347,\n",
       " 'bedevil': 20855,\n",
       " 'dispite': 20856,\n",
       " 'battleships': 16843,\n",
       " 'instability': 4824,\n",
       " 'quarter': 95,\n",
       " 'salado': 20857,\n",
       " 'honduras': 5692,\n",
       " \"chevron's\": 13855,\n",
       " \"lazere's\": 12273,\n",
       " 'receipt': 2660,\n",
       " 'sponsor': 8360,\n",
       " 'entering': 4825,\n",
       " \"kcbt's\": 16349,\n",
       " 'nowicki': 19987,\n",
       " 'salads': 13856,\n",
       " 'augar': 16351,\n",
       " '797': 7980,\n",
       " '796': 7254,\n",
       " '795': 8361,\n",
       " '794': 5295,\n",
       " '793': 5118,\n",
       " '792': 6170,\n",
       " '791': 5296,\n",
       " '790': 4826,\n",
       " \"nikko's\": 20858,\n",
       " 'unsaleable': 20859,\n",
       " '799': 5720,\n",
       " '798': 5693,\n",
       " 'seriously': 2143,\n",
       " 'trauma': 16352,\n",
       " 'tvbh': 20860,\n",
       " 'macedon': 20861,\n",
       " 'disintegrated': 21906,\n",
       " 'adddition': 21909,\n",
       " 'incentives': 2244,\n",
       " 'complicated': 5944,\n",
       " 'reevaluating': 20864,\n",
       " 'thatching': 21921,\n",
       " 'brasil': 7981,\n",
       " '79p': 20865,\n",
       " 'wrong': 4951,\n",
       " 'initiate': 8822,\n",
       " 'aboard': 16353,\n",
       " 'saving': 7255,\n",
       " 'spoken': 8823,\n",
       " 'parkinson': 16364,\n",
       " 'one': 65,\n",
       " 'ont': 20867,\n",
       " 'concert': 7256,\n",
       " \"boston's\": 16354,\n",
       " 'stifled': 13859,\n",
       " 'types': 4622,\n",
       " 'lingering': 20868,\n",
       " 'surges': 16356,\n",
       " 'hurdman': 20869,\n",
       " 'herds': 16357,\n",
       " 'absorbs': 14114,\n",
       " 'surged': 4681,\n",
       " 'dalkon': 14211,\n",
       " 'crossroads': 13860,\n",
       " 'shakeup': 20870,\n",
       " 'disasterous': 20871,\n",
       " 'illness': 11021,\n",
       " 'turned': 3242,\n",
       " 'locations': 3801,\n",
       " 'tyranite': 12120,\n",
       " 'minesweepers': 13861,\n",
       " 'turner': 7257,\n",
       " 'borough': 20872,\n",
       " 'underlines': 12358,\n",
       " \"bancorporation's\": 20873,\n",
       " 'fashionable': 20874,\n",
       " \"ae's\": 20875,\n",
       " 'dilutions': 16358,\n",
       " 'goodman': 9472,\n",
       " 'unlawfully': 10510,\n",
       " 'mayer': 16359,\n",
       " 'printer': 16360,\n",
       " 'offload': 20877,\n",
       " 'opposite': 13862,\n",
       " 'buffer': 738,\n",
       " 'printed': 9398,\n",
       " 'pequiven': 16361,\n",
       " 'panoche': 13863,\n",
       " 'knowingly': 20878,\n",
       " 'ecusta': 16362,\n",
       " 'thsl': 20879,\n",
       " 'phil': 8825,\n",
       " 'jitters': 13864,\n",
       " 'touche': 16363,\n",
       " 'jittery': 20881,\n",
       " 'friction': 3291,\n",
       " 'fecal': 16365,\n",
       " 'resurgance': 22068,\n",
       " 'heeding': 20882,\n",
       " 'soviets': 2363,\n",
       " 'imagined': 16366,\n",
       " 'transact': 16367,\n",
       " 'califoirnia': 20883,\n",
       " \"chrysler's\": 9399,\n",
       " 'respecitvely': 16368,\n",
       " 'presse': 16369,\n",
       " 'euromarket': 10084,\n",
       " 'guarded': 12121,\n",
       " 'satisfacotry': 16371,\n",
       " 'authroization': 20884,\n",
       " 'simplistic': 20885,\n",
       " 'monde': 20886,\n",
       " 'awaiting': 4102,\n",
       " 'recombinant': 13865,\n",
       " 'refinancement': 20887,\n",
       " 'comserv': 20888,\n",
       " 'kitakyushu': 20889,\n",
       " 'pima': 16372,\n",
       " 'basle': 11022,\n",
       " '6250': 20891,\n",
       " 'choudhury': 16373,\n",
       " 'vision': 8826,\n",
       " 'interruptible': 20892,\n",
       " 'weatherford': 13866,\n",
       " '832': 7982,\n",
       " '833': 5694,\n",
       " '830': 4420,\n",
       " '831': 5119,\n",
       " '836': 5297,\n",
       " '837': 4553,\n",
       " '834': 6172,\n",
       " '835': 4952,\n",
       " 'alarming': 22144,\n",
       " '838': 5695,\n",
       " '839': 6173,\n",
       " '524p': 20893,\n",
       " 'sponsorship': 20894,\n",
       " 'vendex': 12122,\n",
       " \"amsouth's\": 20895,\n",
       " 'kilometer': 20896,\n",
       " 'enjoys': 10086,\n",
       " 'illiberal': 20897,\n",
       " 'punta': 6174,\n",
       " 'punte': 20898,\n",
       " 'girozentrale': 10087,\n",
       " 'missstatements': 20899,\n",
       " 'marietta': 10088,\n",
       " 'awards': 6175,\n",
       " 'concentrated': 3635,\n",
       " '83p': 20900,\n",
       " 'developpement': 13867,\n",
       " 'rhodes': 13868,\n",
       " 'matheson': 5696,\n",
       " '1720': 20901,\n",
       " 'paring': 20902,\n",
       " 's': 35,\n",
       " 'concentrates': 4953,\n",
       " \"can's\": 16374,\n",
       " 'polysaturated': 22183,\n",
       " 'parini': 20903,\n",
       " 'baden': 13869,\n",
       " 'bader': 20904,\n",
       " 'buoyancy': 12123,\n",
       " 'erdem': 20905,\n",
       " 'properites': 16375,\n",
       " 'comparitive': 20906,\n",
       " 'practises': 12124,\n",
       " 'collides': 20907,\n",
       " 'west': 189,\n",
       " 'wess': 20908,\n",
       " 'collided': 13870,\n",
       " 'practised': 20909,\n",
       " \"amalgamated's\": 20910,\n",
       " 'motives': 20911,\n",
       " 'wants': 1378,\n",
       " 'formed': 1273,\n",
       " 'readings': 20912,\n",
       " 'geothermal': 12125,\n",
       " 'tightened': 7315,\n",
       " \"d'or\": 11023,\n",
       " 'former': 1109,\n",
       " 'venezulean': 20913,\n",
       " 'curd': 19935,\n",
       " 'squeezes': 12126,\n",
       " 'newspaper': 1019,\n",
       " 'situation': 817,\n",
       " 'ivey': 13871,\n",
       " 'engaged': 3636,\n",
       " 'dubious': 13872,\n",
       " 'cayacq': 17061,\n",
       " 'cobol': 20916,\n",
       " 'limping': 20917,\n",
       " 'technology': 883,\n",
       " 'koerner': 20919,\n",
       " 'debilitating': 16376,\n",
       " 'verified': 7983,\n",
       " 'otto': 4010,\n",
       " '7770': 20920,\n",
       " 'emulsions': 16377,\n",
       " \"onic's\": 16378,\n",
       " 'slate': 9075,\n",
       " 'wires': 20921,\n",
       " 'edged': 5506,\n",
       " 'assigns': 20922,\n",
       " 'singapore': 1341,\n",
       " 'deflate': 20923,\n",
       " \"strategy's\": 20924,\n",
       " 'walesa': 16379,\n",
       " 'advertisement': 4554,\n",
       " 'luyten': 20925,\n",
       " 'shrortly': 20926,\n",
       " 'corpoartion': 20927,\n",
       " 'preferance': 22290,\n",
       " 'tracking': 16380,\n",
       " 'sunnyvale': 13874,\n",
       " 'colorants': 20928,\n",
       " 'persistently': 16381,\n",
       " \"officers'\": 16382,\n",
       " \"his's\": 20929,\n",
       " 'being': 367,\n",
       " 'divestitures': 7259,\n",
       " 'steamer': 20930,\n",
       " 'rover': 20931,\n",
       " 'grounded': 8362,\n",
       " \"businessmen's\": 16383,\n",
       " 'cyanidation': 16384,\n",
       " 'overthrow': 20932,\n",
       " 'partnerhip': 20933,\n",
       " 'sumt': 16385,\n",
       " 'sums': 8827,\n",
       " 'oelmuehle': 16386,\n",
       " 'unveil': 16387,\n",
       " 'gestures': 13875,\n",
       " 'penta': 20934,\n",
       " 'traffic': 2544,\n",
       " 'preference': 2428,\n",
       " 'sumi': 20935,\n",
       " 'world': 166,\n",
       " 'postal': 9400,\n",
       " 'bced': 16388,\n",
       " 'dornbush': 12128,\n",
       " 'confine': 14215,\n",
       " '2555': 20936,\n",
       " \"zambia's\": 5945,\n",
       " 'superiority': 20937,\n",
       " 'militate': 20938,\n",
       " 'satisfactory': 2395,\n",
       " 'superintendent': 20939,\n",
       " 'tvx': 5946,\n",
       " 'tvt': 16389,\n",
       " 'magma': 6698,\n",
       " 'diving': 20940,\n",
       " 'tvb': 15548,\n",
       " 'seaman': 13876,\n",
       " 'matsunaga': 11025,\n",
       " '919': 4827,\n",
       " '918': 5298,\n",
       " 'refundable': 17070,\n",
       " '914': 5947,\n",
       " '917': 7260,\n",
       " '916': 6699,\n",
       " '911': 5507,\n",
       " '910': 4828,\n",
       " 'restoring': 10213,\n",
       " '912': 4555,\n",
       " 'squabble': 20942,\n",
       " 'retains': 7261,\n",
       " \"partner's\": 20943,\n",
       " 'leadership': 5300,\n",
       " 'graaf': 11026,\n",
       " 'spacelab': 20944,\n",
       " 'thailand': 1800,\n",
       " 'graan': 9402,\n",
       " 'exasperating': 20945,\n",
       " 'hartmarx': 12129,\n",
       " 'frights': 16390,\n",
       " 'niall': 20946,\n",
       " 'johnston': 11027,\n",
       " '91p': 16391,\n",
       " 'sensitively': 16392,\n",
       " 'porsche': 6016,\n",
       " 'prepares': 15494,\n",
       " 'lively': 12130,\n",
       " 'stoppages': 10686,\n",
       " \"associated's\": 16394,\n",
       " 'pivot': 12131,\n",
       " 'series': 1037,\n",
       " 'sese': 24050,\n",
       " 'bubble': 7604,\n",
       " 'trusses': 16395,\n",
       " 'interestate': 20949,\n",
       " 'continents': 20950,\n",
       " 'societal': 20951,\n",
       " 'with': 28,\n",
       " 'pull': 6176,\n",
       " 'rush': 6700,\n",
       " 'monopoly': 6222,\n",
       " 'operationally': 20953,\n",
       " 'dirty': 20954,\n",
       " 'abuses': 10090,\n",
       " 'prudhoe': 7262,\n",
       " 'pulp': 5949,\n",
       " 'rust': 16396,\n",
       " 'hellman': 20955,\n",
       " 'amdec': 20956,\n",
       " 'australasian': 16397,\n",
       " 'watches': 13878,\n",
       " 'hypertension': 20957,\n",
       " \"hemdale's\": 20958,\n",
       " 'formulation': 16398,\n",
       " 'watched': 7605,\n",
       " 'jargon': 20959,\n",
       " 'cream': 13879,\n",
       " 'ideally': 9404,\n",
       " 'ryavec': 11028,\n",
       " 'microoganisms': 20960,\n",
       " 'indemnify': 13880,\n",
       " 'wincenty': 20961,\n",
       " 'waving': 20962,\n",
       " \"multifood's\": 20963,\n",
       " 'midges': 20964,\n",
       " 'natalie': 11029,\n",
       " 'crosbie': 13881,\n",
       " 'posible': 20965,\n",
       " 'omnibus': 13882,\n",
       " 'assetsof': 20966,\n",
       " 'tricks': 13883,\n",
       " 'rs': 16399,\n",
       " 'kilogram': 20967,\n",
       " 'pruning': 25363,\n",
       " 'dyer': 13884,\n",
       " 'dyes': 20968,\n",
       " 'legislatures': 20969,\n",
       " 'scm': 16400,\n",
       " 'sci': 9405,\n",
       " 'riedel': 20970,\n",
       " 'ceramic': 16401,\n",
       " 'unitholders': 6701,\n",
       " 'scb': 13885,\n",
       " 'dn11': 20971,\n",
       " 'conditionality': 20972,\n",
       " \"stock's\": 13807,\n",
       " 'masland': 20973,\n",
       " 'causes': 7606,\n",
       " 'riots': 10091,\n",
       " 'norf': 20974,\n",
       " 'nord': 9406,\n",
       " 'midwest': 3893,\n",
       " 'tamils': 13886,\n",
       " 'ofthe': 16402,\n",
       " \"colombia's\": 3421,\n",
       " '24th': 11030,\n",
       " 'sant': 20975,\n",
       " 'moines': 10092,\n",
       " 'electrotechnical': 22577,\n",
       " 'proceeded': 24534,\n",
       " 'sanz': 20976,\n",
       " 'insufficiently': 13887,\n",
       " 'sang': 20977,\n",
       " 'sand': 5950,\n",
       " 'bracho': 16404,\n",
       " 'small': 805,\n",
       " 'workloads': 20978,\n",
       " 'sank': 6702,\n",
       " 'kemper': 20979,\n",
       " 'abbreviated': 16405,\n",
       " 'quicker': 13888,\n",
       " '199': 3802,\n",
       " '198': 3243,\n",
       " '195': 2661,\n",
       " '194': 3080,\n",
       " '197': 4310,\n",
       " '196': 3894,\n",
       " '191': 2850,\n",
       " '190': 2199,\n",
       " '193': 3481,\n",
       " '192': 3350,\n",
       " 'past': 582,\n",
       " 'fractionation': 20980,\n",
       " 'displays': 20981,\n",
       " 'pass': 3081,\n",
       " 'investment': 202,\n",
       " 'quals': 27062,\n",
       " 'quicken': 16406,\n",
       " \"centronic's\": 20983,\n",
       " 'menswear': 20984,\n",
       " 'clock': 16407,\n",
       " 'teape': 20985,\n",
       " 'teapa': 20986,\n",
       " 'prevailed': 10093,\n",
       " 'hebei': 9407,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb052a19",
   "metadata": {},
   "source": [
    "#### 학습\n",
    "- RNN학습을 위해선 뉴스 기사의 길이를 맞춰주어야함\n",
    "- 길이가 긴 뉴스기사는 단어를 잘라서 길이를 축소\n",
    "- 길이가 짧은 뉴스기사는 패딩작업(0을 채움)을 이용해 길이를 확장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e9cd2",
   "metadata": {},
   "source": [
    "##### 시퀀스 길이 확인\n",
    "- 적절한 길이는 무엇일까\n",
    "- 보편적으로 많은것을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c7b2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = [ len(doc) for doc in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6357555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대값 2376\n",
      "최소값 13\n",
      "평균값 145.5398574927633\n",
      "중앙값 95.0\n"
     ]
    }
   ],
   "source": [
    "print('최대값',max(train_len))\n",
    "print('최소값',min(train_len))\n",
    "print('평균값',np.mean(train_len))\n",
    "print('중앙값',np.median(train_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684e65a3",
   "metadata": {},
   "source": [
    "- 시퀀스 길이를 120정도로 맞춰보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c877f19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "668fd78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 길이가 긴건 자르고 짧은건 늘려준다\n",
    "X_train_seq = sequence.pad_sequences(X_train,maxlen = 120)\n",
    "X_test_seq = sequence.pad_sequences(X_test,maxlen = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ad18fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8982, 120), (2246, 120))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq.shape, X_test_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fd168ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     1, 27595, 28842,\n",
       "           8,    43,    10,   447,     5,    25,   207,   270,     5,\n",
       "        3095,   111,    16,   369,   186,    90,    67,     7,    89,\n",
       "           5,    19,   102,     6,    19,   124,    15,    90,    67,\n",
       "          84,    22,   482,    26,     7,    48,     4,    49,     8,\n",
       "         864,    39,   209,   154,     6,   151,     6,    83,    11,\n",
       "          15,    22,   155,    11,    15,     7,    48,     9,  4579,\n",
       "        1005,   504,     6,   258,     6,   272,    11,    15,    22,\n",
       "         134,    44,    11,    15,    16,     8,   197,  1245,    90,\n",
       "          67,    52,    29,   209,    30,    32,   132,     6,   109,\n",
       "          15,    17,    12])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bd156ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq_reshape = X_train_seq.reshape(8982,120,1)\n",
    "X_test_seq_reshape = X_test_seq.reshape(2246,120,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fce03142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8982, 120, 1), (2246, 120, 1))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq_reshape.shape, X_test_seq_reshape.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8d5359",
   "metadata": {},
   "source": [
    "모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e712c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "471b28f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn (SimpleRNN)       (None, 128)               16640     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 27,886\n",
      "Trainable params: 27,886\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "model1.add(SimpleRNN(128, input_shape=(120,1))) #120개길이, 단어하나\n",
    "\n",
    "model1.add(Dense(64,activation = 'relu'))\n",
    "model1.add(Dense(46,activation = 'softmax')) # 0~45클래스중 하나의 값 출력\n",
    "                                            #46개의 결과값을 출력하고 각각\n",
    "                                            #몇퍼센트 해당하는지 알아보기위해서\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7391583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss = 'sparse_categorical_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d1214cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 모델을 저장할 폴더명\n",
    "MODEL_FOLDER = './model'\n",
    "\n",
    "# 해당 폴더가 없다면 해당 폴더를 생성\n",
    "if not os.path.exists(MODEL_FOLDER) :\n",
    "    os.mkdir(MODEL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a89d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장할 파일 명 설정\n",
    " #{epoch:04d} : 반복수를 4자리로 표시\n",
    "# {val_accuracy:.4f}:검증 정확도를 소수점 4째자리까지 표시\n",
    "#hdf5 파일형식\n",
    "modelpath = MODEL_FOLDER + './reuters-{epoch:04d}-{val_accuracy:4f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1767c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베스트 모델을 찾아서 만들어둔 파일 명으로 저장\n",
    "# ModelCheckpoint(filepath = 파일 경로, monitor = 기준값,save_bast_only=True)\n",
    "#save_bast_only=True : 더 나은 결과값만 저장\n",
    "mc = ModelCheckpoint(filepath = modelpath,\n",
    "                    monitor = 'val_accuracy',\n",
    "                    save_best_only = True,\n",
    "                    verbose = 1) #verbose=1 진행결과를 보지 않겠다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57556ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping(monitor,patience = 기다리는 횟수)\n",
    "# patience = 20 : monitor 에 적은 기준에 따라 학습 결과가 더 나아지지 않더라도 \n",
    "#20번은 돌려보겠다.\n",
    "# patience가 있어야지만 조금씩 나아진 결과를 확인할 수 있다.\n",
    "#EarlyStopping 한 번 안좋아지면 멈춤\n",
    "es = EarlyStopping(monitor='val_accuracy',\n",
    "                  patience = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c5be68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7185 samples, validate on 1797 samples\n",
      "Epoch 1/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 1.9448 - accuracy: 0.4227\n",
      "Epoch 00001: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 6s 903us/sample - loss: 1.9448 - accuracy: 0.4225 - val_loss: 2.4979 - val_accuracy: 0.3706\n",
      "Epoch 2/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9333 - accuracy: 0.4281\n",
      "Epoch 00002: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 959us/sample - loss: 1.9330 - accuracy: 0.4280 - val_loss: 2.5312 - val_accuracy: 0.3734\n",
      "Epoch 3/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 1.9330 - accuracy: 0.4251\n",
      "Epoch 00003: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 919us/sample - loss: 1.9330 - accuracy: 0.4253 - val_loss: 2.5365 - val_accuracy: 0.3756\n",
      "Epoch 4/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9384 - accuracy: 0.4290\n",
      "Epoch 00004: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 939us/sample - loss: 1.9377 - accuracy: 0.4292 - val_loss: 2.5090 - val_accuracy: 0.3767\n",
      "Epoch 5/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9230 - accuracy: 0.4318\n",
      "Epoch 00005: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 928us/sample - loss: 1.9238 - accuracy: 0.4322 - val_loss: 2.5131 - val_accuracy: 0.3767\n",
      "Epoch 6/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9251 - accuracy: 0.4278\n",
      "Epoch 00006: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 938us/sample - loss: 1.9244 - accuracy: 0.4277 - val_loss: 2.5081 - val_accuracy: 0.3717\n",
      "Epoch 7/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 1.9360 - accuracy: 0.4311\n",
      "Epoch 00007: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 938us/sample - loss: 1.9359 - accuracy: 0.4308 - val_loss: 2.5373 - val_accuracy: 0.3756\n",
      "Epoch 8/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 1.9514 - accuracy: 0.4262\n",
      "Epoch 00008: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 938us/sample - loss: 1.9518 - accuracy: 0.4262 - val_loss: 2.5323 - val_accuracy: 0.3806\n",
      "Epoch 9/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9213 - accuracy: 0.4336\n",
      "Epoch 00009: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 940us/sample - loss: 1.9237 - accuracy: 0.4328 - val_loss: 2.5377 - val_accuracy: 0.3790\n",
      "Epoch 10/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 1.9176 - accuracy: 0.4328\n",
      "Epoch 00010: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 929us/sample - loss: 1.9195 - accuracy: 0.4324 - val_loss: 2.5663 - val_accuracy: 0.3684\n",
      "Epoch 11/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9209 - accuracy: 0.4362\n",
      "Epoch 00011: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 950us/sample - loss: 1.9213 - accuracy: 0.4358 - val_loss: 2.5857 - val_accuracy: 0.3812\n",
      "Epoch 12/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 1.9154 - accuracy: 0.4344\n",
      "Epoch 00012: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 936us/sample - loss: 1.9152 - accuracy: 0.4347 - val_loss: 2.5349 - val_accuracy: 0.3745\n",
      "Epoch 13/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9392 - accuracy: 0.4330\n",
      "Epoch 00013: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 949us/sample - loss: 1.9411 - accuracy: 0.4326 - val_loss: 2.6007 - val_accuracy: 0.3600\n",
      "Epoch 14/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9481 - accuracy: 0.4297\n",
      "Epoch 00014: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 969us/sample - loss: 1.9486 - accuracy: 0.4289 - val_loss: 2.5834 - val_accuracy: 0.3723\n",
      "Epoch 15/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9262 - accuracy: 0.4304\n",
      "Epoch 00015: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 983us/sample - loss: 1.9267 - accuracy: 0.4298 - val_loss: 2.5497 - val_accuracy: 0.3662\n",
      "Epoch 16/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9247 - accuracy: 0.4313\n",
      "Epoch 00016: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 962us/sample - loss: 1.9251 - accuracy: 0.4310 - val_loss: 2.5513 - val_accuracy: 0.3790\n",
      "Epoch 17/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9145 - accuracy: 0.4337\n",
      "Epoch 00017: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 951us/sample - loss: 1.9136 - accuracy: 0.4340 - val_loss: 2.5677 - val_accuracy: 0.3706\n",
      "Epoch 18/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9151 - accuracy: 0.4323\n",
      "Epoch 00018: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 965us/sample - loss: 1.9134 - accuracy: 0.4330 - val_loss: 2.5571 - val_accuracy: 0.3762\n",
      "Epoch 19/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9341 - accuracy: 0.4346\n",
      "Epoch 00019: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 954us/sample - loss: 1.9368 - accuracy: 0.4341 - val_loss: 2.6471 - val_accuracy: 0.3734\n",
      "Epoch 20/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9336 - accuracy: 0.4297\n",
      "Epoch 00020: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 960us/sample - loss: 1.9335 - accuracy: 0.4295 - val_loss: 2.6039 - val_accuracy: 0.3723\n",
      "Epoch 21/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9118 - accuracy: 0.4316\n",
      "Epoch 00021: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 997us/sample - loss: 1.9135 - accuracy: 0.4313 - val_loss: 2.5751 - val_accuracy: 0.3779\n",
      "Epoch 22/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9107 - accuracy: 0.4357\n",
      "Epoch 00022: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 969us/sample - loss: 1.9119 - accuracy: 0.4359 - val_loss: 2.5944 - val_accuracy: 0.3667\n",
      "Epoch 23/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9241 - accuracy: 0.4381\n",
      "Epoch 00023: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 952us/sample - loss: 1.9242 - accuracy: 0.4374 - val_loss: 2.6046 - val_accuracy: 0.3728\n",
      "Epoch 24/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9267 - accuracy: 0.4334\n",
      "Epoch 00024: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 974us/sample - loss: 1.9254 - accuracy: 0.4338 - val_loss: 2.6051 - val_accuracy: 0.3767\n",
      "Epoch 25/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9144 - accuracy: 0.4254\n",
      "Epoch 00025: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 965us/sample - loss: 1.9164 - accuracy: 0.4255 - val_loss: 2.5848 - val_accuracy: 0.3734\n",
      "Epoch 26/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9174 - accuracy: 0.4361\n",
      "Epoch 00026: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 967us/sample - loss: 1.9171 - accuracy: 0.4363 - val_loss: 2.5428 - val_accuracy: 0.3751\n",
      "Epoch 27/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9127 - accuracy: 0.4354\n",
      "Epoch 00027: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 968us/sample - loss: 1.9119 - accuracy: 0.4359 - val_loss: 2.6298 - val_accuracy: 0.3701\n",
      "Epoch 28/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9039 - accuracy: 0.4381\n",
      "Epoch 00028: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 968us/sample - loss: 1.9057 - accuracy: 0.4370 - val_loss: 2.5544 - val_accuracy: 0.3728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.8994 - accuracy: 0.4361 ETA: 0s - loss: 1.9015 - accura\n",
      "Epoch 00029: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 966us/sample - loss: 1.8974 - accuracy: 0.4373 - val_loss: 2.5875 - val_accuracy: 0.3734\n",
      "Epoch 30/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.8957 - accuracy: 0.4368\n",
      "Epoch 00030: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 966us/sample - loss: 1.8964 - accuracy: 0.4369 - val_loss: 2.6495 - val_accuracy: 0.3734\n",
      "Epoch 31/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9106 - accuracy: 0.4351\n",
      "Epoch 00031: val_accuracy did not improve from 0.38230\n",
      "7185/7185 [==============================] - 7s 967us/sample - loss: 1.9113 - accuracy: 0.4347 - val_loss: 2.5949 - val_accuracy: 0.3712\n"
     ]
    }
   ],
   "source": [
    "h1 = model1.fit(X_train_seq_reshape,y_train,\n",
    "               validation_split = 0.2,\n",
    "               epochs = 100,callbacks=[mc,es])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc26c10",
   "metadata": {},
   "source": [
    "#### 모델링2: RNN레이어 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29f58314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_15 (SimpleRNN)    (None, 120, 128)          16640     \n",
      "_________________________________________________________________\n",
      "simple_rnn_16 (SimpleRNN)    (None, 120, 64)           12352     \n",
      "_________________________________________________________________\n",
      "simple_rnn_17 (SimpleRNN)    (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 44,398\n",
      "Trainable params: 44,398\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "# return_sequeces = True : 뉴런마다 똑같은 데이터가 들어갈 수 있게 한다\n",
    "model2.add(SimpleRNN(128, input_shape=(120,1),return_sequences = True)) #120개길이, 단어하나\n",
    "model2.add(SimpleRNN(64, return_sequences = True))\n",
    "model2.add(SimpleRNN(64))\n",
    "\n",
    "# 기본적으로 모든 층이 다수입력 단일출력\n",
    "# RNN을 여러 층을 쌓으려고 생각한다면 각 층을 다수입력 다수출력으로 바꿔야함\n",
    "# 분류기로 넘겨줄때 특성값을 넘겨줘야 하기 때문에\n",
    "# return_sequences 가 필요없음\n",
    "\n",
    "model2.add(Dense(64,activation = 'relu'))\n",
    "model2.add(Dense(46,activation = 'softmax')) # 0~45클래스중 하나의 값 출력\n",
    "                                            #46개의 결과값을 출력하고 각각\n",
    "                                            #몇퍼센트 해당하는지 알아보기위해서\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e92582eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss = 'sparse_categorical_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e33c767b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7185 samples, validate on 1797 samples\n",
      "Epoch 1/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.4434 - accuracy: 0.3606\n",
      "Epoch 00001: val_accuracy did not improve from 0.43795\n",
      "7185/7185 [==============================] - 21s 3ms/sample - loss: 2.4421 - accuracy: 0.3610 - val_loss: 2.3775 - val_accuracy: 0.3667\n",
      "Epoch 2/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.3309 - accuracy: 0.3906\n",
      "Epoch 00002: val_accuracy did not improve from 0.43795\n",
      "7185/7185 [==============================] - 20s 3ms/sample - loss: 2.3307 - accuracy: 0.3908 - val_loss: 2.3891 - val_accuracy: 0.3851\n",
      "Epoch 3/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.3487 - accuracy: 0.3832\n",
      "Epoch 00003: val_accuracy did not improve from 0.43795\n",
      "7185/7185 [==============================] - 20s 3ms/sample - loss: 2.3501 - accuracy: 0.3830 - val_loss: 2.4121 - val_accuracy: 0.3450\n",
      "Epoch 4/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.3598 - accuracy: 0.3583\n",
      "Epoch 00004: val_accuracy did not improve from 0.43795\n",
      "7185/7185 [==============================] - 20s 3ms/sample - loss: 2.3596 - accuracy: 0.3581 - val_loss: 2.3327 - val_accuracy: 0.3656\n",
      "Epoch 5/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.3229 - accuracy: 0.3817\n",
      "Epoch 00005: val_accuracy did not improve from 0.43795\n",
      "7185/7185 [==============================] - 20s 3ms/sample - loss: 2.3229 - accuracy: 0.3819 - val_loss: 2.3414 - val_accuracy: 0.3745\n",
      "Epoch 6/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.2952 - accuracy: 0.3890\n",
      "Epoch 00006: val_accuracy improved from 0.43795 to 0.44964, saving model to ./model./reuters-0006-0.449638.hdf5\n",
      "7185/7185 [==============================] - 19s 3ms/sample - loss: 2.2953 - accuracy: 0.3889 - val_loss: 2.1755 - val_accuracy: 0.4496\n",
      "Epoch 7/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.3599 - accuracy: 0.3573\n",
      "Epoch 00007: val_accuracy did not improve from 0.44964\n",
      "7185/7185 [==============================] - 20s 3ms/sample - loss: 2.3590 - accuracy: 0.3574 - val_loss: 2.3340 - val_accuracy: 0.3467\n",
      "Epoch 8/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.3027 - accuracy: 0.3714\n",
      "Epoch 00008: val_accuracy did not improve from 0.44964\n",
      "7185/7185 [==============================] - 20s 3ms/sample - loss: 2.3028 - accuracy: 0.3712 - val_loss: 2.2822 - val_accuracy: 0.3612\n",
      "Epoch 9/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.2663 - accuracy: 0.3848\n",
      "Epoch 00009: val_accuracy did not improve from 0.44964\n",
      "7185/7185 [==============================] - 19s 3ms/sample - loss: 2.2666 - accuracy: 0.3844 - val_loss: 2.2918 - val_accuracy: 0.3645\n",
      "Epoch 10/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.2470 - accuracy: 0.3955\n",
      "Epoch 00010: val_accuracy did not improve from 0.44964\n",
      "7185/7185 [==============================] - 19s 3ms/sample - loss: 2.2473 - accuracy: 0.3957 - val_loss: 2.2792 - val_accuracy: 0.3801\n",
      "Epoch 11/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.2107 - accuracy: 0.4124\n",
      "Epoch 00011: val_accuracy did not improve from 0.44964\n",
      "7185/7185 [==============================] - 20s 3ms/sample - loss: 2.2113 - accuracy: 0.4124 - val_loss: 2.2196 - val_accuracy: 0.4057\n",
      "Epoch 12/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.2044 - accuracy: 0.4170\n",
      "Epoch 00012: val_accuracy improved from 0.44964 to 0.45409, saving model to ./model./reuters-0012-0.454090.hdf5\n",
      "7185/7185 [==============================] - 21s 3ms/sample - loss: 2.2034 - accuracy: 0.4175 - val_loss: 2.1242 - val_accuracy: 0.4541\n",
      "Epoch 13/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.2204 - accuracy: 0.4100\n",
      "Epoch 00013: val_accuracy did not improve from 0.45409\n",
      "7185/7185 [==============================] - 20s 3ms/sample - loss: 2.2197 - accuracy: 0.4104 - val_loss: 2.2789 - val_accuracy: 0.3723\n",
      "Epoch 14/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.2216 - accuracy: 0.4050\n",
      "Epoch 00014: val_accuracy did not improve from 0.45409\n",
      "7185/7185 [==============================] - 20s 3ms/sample - loss: 2.2215 - accuracy: 0.4051 - val_loss: 2.2221 - val_accuracy: 0.3940\n",
      "Epoch 15/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.2124 - accuracy: 0.4150\n",
      "Epoch 00015: val_accuracy did not improve from 0.45409\n",
      "7185/7185 [==============================] - 19s 3ms/sample - loss: 2.2116 - accuracy: 0.4152 - val_loss: 2.1987 - val_accuracy: 0.4151\n",
      "Epoch 16/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.2383 - accuracy: 0.3973\n",
      "Epoch 00016: val_accuracy did not improve from 0.45409\n",
      "7185/7185 [==============================] - 19s 3ms/sample - loss: 2.2369 - accuracy: 0.3979 - val_loss: 2.1901 - val_accuracy: 0.4235\n",
      "Epoch 17/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.2162 - accuracy: 0.4103\n",
      "Epoch 00017: val_accuracy did not improve from 0.45409\n",
      "7185/7185 [==============================] - 20s 3ms/sample - loss: 2.2153 - accuracy: 0.4102 - val_loss: 2.2038 - val_accuracy: 0.4140\n",
      "Epoch 18/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.2587 - accuracy: 0.3902\n",
      "Epoch 00018: val_accuracy did not improve from 0.45409\n",
      "7185/7185 [==============================] - 19s 3ms/sample - loss: 2.2584 - accuracy: 0.3901 - val_loss: 2.2579 - val_accuracy: 0.3728\n",
      "Epoch 19/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.2067 - accuracy: 0.3996\n",
      "Epoch 00019: val_accuracy did not improve from 0.45409\n",
      "7185/7185 [==============================] - 19s 3ms/sample - loss: 2.2050 - accuracy: 0.4003 - val_loss: 2.2483 - val_accuracy: 0.3823\n",
      "Epoch 20/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.1851 - accuracy: 0.4079\n",
      "Epoch 00020: val_accuracy did not improve from 0.45409\n",
      "7185/7185 [==============================] - 19s 3ms/sample - loss: 2.1857 - accuracy: 0.4074 - val_loss: 2.2295 - val_accuracy: 0.3929\n",
      "Epoch 21/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.1684 - accuracy: 0.4120\n",
      "Epoch 00021: val_accuracy did not improve from 0.45409\n",
      "7185/7185 [==============================] - 19s 3ms/sample - loss: 2.1695 - accuracy: 0.4120 - val_loss: 2.2301 - val_accuracy: 0.3934\n",
      "Epoch 22/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.1592 - accuracy: 0.4182\n",
      "Epoch 00022: val_accuracy did not improve from 0.45409\n",
      "7185/7185 [==============================] - 19s 3ms/sample - loss: 2.1597 - accuracy: 0.4184 - val_loss: 2.2373 - val_accuracy: 0.3845\n",
      "Epoch 23/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.1472 - accuracy: 0.4194\n",
      "Epoch 00023: val_accuracy did not improve from 0.45409\n",
      "7185/7185 [==============================] - 19s 3ms/sample - loss: 2.1489 - accuracy: 0.4191 - val_loss: 2.2194 - val_accuracy: 0.4012\n",
      "Epoch 24/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.1364 - accuracy: 0.4247\n",
      "Epoch 00024: val_accuracy did not improve from 0.45409\n",
      "7185/7185 [==============================] - 19s 3ms/sample - loss: 2.1370 - accuracy: 0.4246 - val_loss: 2.2272 - val_accuracy: 0.4079\n",
      "Epoch 25/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.1210 - accuracy: 0.4283\n",
      "Epoch 00025: val_accuracy did not improve from 0.45409\n",
      "7185/7185 [==============================] - 19s 3ms/sample - loss: 2.1211 - accuracy: 0.4284 - val_loss: 2.2162 - val_accuracy: 0.4018\n",
      "Epoch 26/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.1154 - accuracy: 0.4289\n",
      "Epoch 00026: val_accuracy did not improve from 0.45409\n",
      "7185/7185 [==============================] - 19s 3ms/sample - loss: 2.1144 - accuracy: 0.4289 - val_loss: 2.2305 - val_accuracy: 0.4107\n",
      "Epoch 27/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.1126 - accuracy: 0.4357\n",
      "Epoch 00027: val_accuracy did not improve from 0.45409\n",
      "7185/7185 [==============================] - 19s 3ms/sample - loss: 2.1132 - accuracy: 0.4355 - val_loss: 2.2256 - val_accuracy: 0.4162\n",
      "Epoch 28/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.1070 - accuracy: 0.4351\n",
      "Epoch 00028: val_accuracy did not improve from 0.45409\n",
      "7185/7185 [==============================] - 19s 3ms/sample - loss: 2.1079 - accuracy: 0.4351 - val_loss: 2.1793 - val_accuracy: 0.4341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.0895 - accuracy: 0.4515\n",
      "Epoch 00029: val_accuracy did not improve from 0.45409\n",
      "7185/7185 [==============================] - 19s 3ms/sample - loss: 2.0893 - accuracy: 0.4514 - val_loss: 2.2248 - val_accuracy: 0.4374\n",
      "Epoch 30/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.1060 - accuracy: 0.4406\n",
      "Epoch 00030: val_accuracy did not improve from 0.45409\n",
      "7185/7185 [==============================] - 19s 3ms/sample - loss: 2.1052 - accuracy: 0.4405 - val_loss: 2.1832 - val_accuracy: 0.4418\n",
      "Epoch 31/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.1971 - accuracy: 0.4129\n",
      "Epoch 00031: val_accuracy did not improve from 0.45409\n",
      "7185/7185 [==============================] - 19s 3ms/sample - loss: 2.1981 - accuracy: 0.4129 - val_loss: 2.3171 - val_accuracy: 0.3779\n",
      "Epoch 32/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.1918 - accuracy: 0.4107\n",
      "Epoch 00032: val_accuracy did not improve from 0.45409\n",
      "7185/7185 [==============================] - 19s 3ms/sample - loss: 2.1918 - accuracy: 0.4113 - val_loss: 2.2044 - val_accuracy: 0.4140\n"
     ]
    }
   ],
   "source": [
    "h2 = model2.fit(X_train_seq_reshape,y_train,\n",
    "               validation_split = 0.2,\n",
    "               epochs = 100,callbacks=[mc,es])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccfb899",
   "metadata": {},
   "source": [
    "- 다수입력 다수출력 > 영상 프레임마다 폭력이 있는지 없는지 구분\n",
    "- 다수입력 단일출력 > 영상 내에서 폭력이 있는지 없는지 구분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6b9bad",
   "metadata": {},
   "source": [
    "RNN의 문제점\n",
    "- 데이터가 길어지면 기억값을 전달하는 과정에서 기억값이 소실이 됨\n",
    "- LSTM 긴 기간동안 기억을 하는 학습을 수행하는 능력을 가짐 \n",
    "- 중요한 기억에는 가중치부여, 중요하지 않은 기억에는 강중치를 덜 부여 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1160e5f",
   "metadata": {},
   "source": [
    "##### 모델링3: LSTM 레이어 추가하기(장단기 기억 메모리)\n",
    "- 시퀀스 길이가 길어질수록 RNN 은 기억값이 소실되는 문제가 발생 \n",
    "- 기억을 관리하는 LSTM은 이와같은 경우에서 성능을 끌어올릴 수 있다\n",
    "- 중요한 기억은 강조, 불필요한 기억은 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee20c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cd2d1af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 120, 128)          66560     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 120, 64)           49408     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 156,142\n",
      "Trainable params: 156,142\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "# return_sequeces = True : 뉴런마다 똑같은 데이터가 들어갈 수 있게 한다\n",
    "model3.add(LSTM(128, input_shape=(120,1),return_sequences = True)) #120개길이, 단어하나\n",
    "model3.add(LSTM(64, return_sequences = True))\n",
    "model3.add(LSTM(64))\n",
    "\n",
    "# 기본적으로 모든 층이 다수입력 단일출력\n",
    "# RNN을 여러 층을 쌓으려고 생각한다면 각 층을 다수입력 다수출력으로 바꿔야함\n",
    "# 분류기로 넘겨줄때 특성값을 넘겨줘야 하기 때문에\n",
    "# return_sequences 가 필요없음\n",
    "\n",
    "model3.add(Dense(64,activation = 'relu'))\n",
    "model3.add(Dense(46,activation = 'softmax')) # 0~45클래스중 하나의 값 출력\n",
    "                                            #46개의 결과값을 출력하고 각각\n",
    "                                            #몇퍼센트 해당하는지 알아보기위해서\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d34d0d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(loss = 'sparse_categorical_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "324fb80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7185 samples, validate on 1797 samples\n",
      "Epoch 1/100\n",
      "7104/7185 [============================>.] - ETA: 0s - loss: 2.3233 - accuracy: 0.4091\n",
      "Epoch 00001: val_accuracy improved from 0.45409 to 0.46244, saving model to ./model./reuters-0001-0.462437.hdf5\n",
      "7185/7185 [==============================] - 9s 1ms/sample - loss: 2.3197 - accuracy: 0.4103 - val_loss: 2.1438 - val_accuracy: 0.4624\n",
      "Epoch 2/100\n",
      "7104/7185 [============================>.] - ETA: 0s - loss: 2.1260 - accuracy: 0.4668\n",
      "Epoch 00002: val_accuracy improved from 0.46244 to 0.47635, saving model to ./model./reuters-0002-0.476349.hdf5\n",
      "7185/7185 [==============================] - 3s 457us/sample - loss: 2.1245 - accuracy: 0.4678 - val_loss: 2.1294 - val_accuracy: 0.4763\n",
      "Epoch 3/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.0972 - accuracy: 0.4639\n",
      "Epoch 00003: val_accuracy did not improve from 0.47635\n",
      "7185/7185 [==============================] - 3s 448us/sample - loss: 2.0962 - accuracy: 0.4643 - val_loss: 2.0773 - val_accuracy: 0.4519\n",
      "Epoch 4/100\n",
      "7104/7185 [============================>.] - ETA: 0s - loss: 2.0635 - accuracy: 0.4654\n",
      "Epoch 00004: val_accuracy did not improve from 0.47635\n",
      "7185/7185 [==============================] - 3s 453us/sample - loss: 2.0647 - accuracy: 0.4650 - val_loss: 2.0917 - val_accuracy: 0.4508\n",
      "Epoch 5/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 2.0486 - accuracy: 0.4724\n",
      "Epoch 00005: val_accuracy did not improve from 0.47635\n",
      "7185/7185 [==============================] - 3s 451us/sample - loss: 2.0497 - accuracy: 0.4720 - val_loss: 2.0525 - val_accuracy: 0.4752\n",
      "Epoch 6/100\n",
      "7104/7185 [============================>.] - ETA: 0s - loss: 2.0336 - accuracy: 0.4713\n",
      "Epoch 00006: val_accuracy did not improve from 0.47635\n",
      "7185/7185 [==============================] - 3s 442us/sample - loss: 2.0375 - accuracy: 0.4707 - val_loss: 2.0245 - val_accuracy: 0.4636\n",
      "Epoch 7/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.0234 - accuracy: 0.4708\n",
      "Epoch 00007: val_accuracy did not improve from 0.47635\n",
      "7185/7185 [==============================] - 3s 441us/sample - loss: 2.0236 - accuracy: 0.4710 - val_loss: 2.0161 - val_accuracy: 0.4708\n",
      "Epoch 8/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 2.0077 - accuracy: 0.4798\n",
      "Epoch 00008: val_accuracy did not improve from 0.47635\n",
      "7185/7185 [==============================] - 3s 442us/sample - loss: 2.0089 - accuracy: 0.4789 - val_loss: 2.0092 - val_accuracy: 0.4680\n",
      "Epoch 9/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 1.9958 - accuracy: 0.4782\n",
      "Epoch 00009: val_accuracy improved from 0.47635 to 0.47802, saving model to ./model./reuters-0009-0.478019.hdf5\n",
      "7185/7185 [==============================] - 3s 441us/sample - loss: 1.9945 - accuracy: 0.4788 - val_loss: 1.9885 - val_accuracy: 0.4780\n",
      "Epoch 10/100\n",
      "7104/7185 [============================>.] - ETA: 0s - loss: 1.9890 - accuracy: 0.4782\n",
      "Epoch 00010: val_accuracy did not improve from 0.47802\n",
      "7185/7185 [==============================] - 3s 453us/sample - loss: 1.9865 - accuracy: 0.4788 - val_loss: 2.0207 - val_accuracy: 0.4719\n",
      "Epoch 11/100\n",
      "7104/7185 [============================>.] - ETA: 0s - loss: 1.9691 - accuracy: 0.4849\n",
      "Epoch 00011: val_accuracy did not improve from 0.47802\n",
      "7185/7185 [==============================] - 3s 442us/sample - loss: 1.9687 - accuracy: 0.4850 - val_loss: 1.9643 - val_accuracy: 0.4775\n",
      "Epoch 12/100\n",
      "7104/7185 [============================>.] - ETA: 0s - loss: 1.9631 - accuracy: 0.4838\n",
      "Epoch 00012: val_accuracy did not improve from 0.47802\n",
      "7185/7185 [==============================] - 3s 437us/sample - loss: 1.9660 - accuracy: 0.4828 - val_loss: 1.9732 - val_accuracy: 0.4758\n",
      "Epoch 13/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.9477 - accuracy: 0.4823\n",
      "Epoch 00013: val_accuracy did not improve from 0.47802\n",
      "7185/7185 [==============================] - 3s 438us/sample - loss: 1.9520 - accuracy: 0.4816 - val_loss: 1.9272 - val_accuracy: 0.4780\n",
      "Epoch 14/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.9152 - accuracy: 0.4857\n",
      "Epoch 00014: val_accuracy improved from 0.47802 to 0.48191, saving model to ./model./reuters-0014-0.481914.hdf5\n",
      "7185/7185 [==============================] - 3s 441us/sample - loss: 1.9150 - accuracy: 0.4860 - val_loss: 1.9156 - val_accuracy: 0.4819\n",
      "Epoch 15/100\n",
      "7104/7185 [============================>.] - ETA: 0s - loss: 1.8965 - accuracy: 0.4914\n",
      "Epoch 00015: val_accuracy improved from 0.48191 to 0.49082, saving model to ./model./reuters-0015-0.490818.hdf5\n",
      "7185/7185 [==============================] - 3s 441us/sample - loss: 1.8954 - accuracy: 0.4916 - val_loss: 1.9007 - val_accuracy: 0.4908\n",
      "Epoch 16/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.8720 - accuracy: 0.4948\n",
      "Epoch 00016: val_accuracy did not improve from 0.49082\n",
      "7185/7185 [==============================] - 3s 440us/sample - loss: 1.8748 - accuracy: 0.4942 - val_loss: 1.9811 - val_accuracy: 0.4563\n",
      "Epoch 17/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.8518 - accuracy: 0.4972\n",
      "Epoch 00017: val_accuracy did not improve from 0.49082\n",
      "7185/7185 [==============================] - 3s 446us/sample - loss: 1.8509 - accuracy: 0.4974 - val_loss: 1.9360 - val_accuracy: 0.4691\n",
      "Epoch 18/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.8157 - accuracy: 0.5098\n",
      "Epoch 00018: val_accuracy improved from 0.49082 to 0.50250, saving model to ./model./reuters-0018-0.502504.hdf5\n",
      "7185/7185 [==============================] - 3s 440us/sample - loss: 1.8165 - accuracy: 0.5104 - val_loss: 1.8620 - val_accuracy: 0.5025\n",
      "Epoch 19/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.8026 - accuracy: 0.5170\n",
      "Epoch 00019: val_accuracy improved from 0.50250 to 0.50584, saving model to ./model./reuters-0019-0.505843.hdf5\n",
      "7185/7185 [==============================] - 3s 443us/sample - loss: 1.8023 - accuracy: 0.5166 - val_loss: 1.8378 - val_accuracy: 0.5058\n",
      "Epoch 20/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.7844 - accuracy: 0.5184\n",
      "Epoch 00020: val_accuracy did not improve from 0.50584\n",
      "7185/7185 [==============================] - 3s 459us/sample - loss: 1.7830 - accuracy: 0.5186 - val_loss: 1.8889 - val_accuracy: 0.4975\n",
      "Epoch 21/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.7670 - accuracy: 0.5212\n",
      "Epoch 00021: val_accuracy improved from 0.50584 to 0.50974, saving model to ./model./reuters-0021-0.509738.hdf5\n",
      "7185/7185 [==============================] - 3s 445us/sample - loss: 1.7667 - accuracy: 0.5214 - val_loss: 1.8365 - val_accuracy: 0.5097\n",
      "Epoch 22/100\n",
      "7104/7185 [============================>.] - ETA: 0s - loss: 1.7505 - accuracy: 0.5251\n",
      "Epoch 00022: val_accuracy improved from 0.50974 to 0.51697, saving model to ./model./reuters-0022-0.516973.hdf5\n",
      "7185/7185 [==============================] - 3s 445us/sample - loss: 1.7504 - accuracy: 0.5254 - val_loss: 1.8252 - val_accuracy: 0.5170\n",
      "Epoch 23/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 1.7268 - accuracy: 0.5286\n",
      "Epoch 00023: val_accuracy did not improve from 0.51697\n",
      "7185/7185 [==============================] - 3s 445us/sample - loss: 1.7271 - accuracy: 0.5286 - val_loss: 1.8225 - val_accuracy: 0.5170\n",
      "Epoch 24/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.7263 - accuracy: 0.5305\n",
      "Epoch 00024: val_accuracy did not improve from 0.51697\n",
      "7185/7185 [==============================] - 3s 443us/sample - loss: 1.7261 - accuracy: 0.5301 - val_loss: 1.8162 - val_accuracy: 0.5131\n",
      "Epoch 25/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 1.6927 - accuracy: 0.5393\n",
      "Epoch 00025: val_accuracy improved from 0.51697 to 0.52476, saving model to ./model./reuters-0025-0.524763.hdf5\n",
      "7185/7185 [==============================] - 3s 447us/sample - loss: 1.6931 - accuracy: 0.5392 - val_loss: 1.8058 - val_accuracy: 0.5248\n",
      "Epoch 26/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.6806 - accuracy: 0.5407\n",
      "Epoch 00026: val_accuracy did not improve from 0.52476\n",
      "7185/7185 [==============================] - 3s 445us/sample - loss: 1.6822 - accuracy: 0.5400 - val_loss: 1.7968 - val_accuracy: 0.5131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.6797 - accuracy: 0.5431\n",
      "Epoch 00027: val_accuracy improved from 0.52476 to 0.52643, saving model to ./model./reuters-0027-0.526433.hdf5\n",
      "7185/7185 [==============================] - 3s 437us/sample - loss: 1.6749 - accuracy: 0.5438 - val_loss: 1.8031 - val_accuracy: 0.5264\n",
      "Epoch 28/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.6481 - accuracy: 0.5497\n",
      "Epoch 00028: val_accuracy improved from 0.52643 to 0.53422, saving model to ./model./reuters-0028-0.534224.hdf5\n",
      "7185/7185 [==============================] - 3s 437us/sample - loss: 1.6493 - accuracy: 0.5493 - val_loss: 1.7916 - val_accuracy: 0.5342\n",
      "Epoch 29/100\n",
      "7104/7185 [============================>.] - ETA: 0s - loss: 1.6387 - accuracy: 0.5501\n",
      "Epoch 00029: val_accuracy did not improve from 0.53422\n",
      "7185/7185 [==============================] - 3s 435us/sample - loss: 1.6398 - accuracy: 0.5503 - val_loss: 1.7840 - val_accuracy: 0.5192\n",
      "Epoch 30/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.6217 - accuracy: 0.5566\n",
      "Epoch 00030: val_accuracy did not improve from 0.53422\n",
      "7185/7185 [==============================] - 3s 436us/sample - loss: 1.6221 - accuracy: 0.5570 - val_loss: 1.8076 - val_accuracy: 0.5337\n",
      "Epoch 31/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.6040 - accuracy: 0.5582\n",
      "Epoch 00031: val_accuracy did not improve from 0.53422\n",
      "7185/7185 [==============================] - 3s 436us/sample - loss: 1.6046 - accuracy: 0.5577 - val_loss: 1.8014 - val_accuracy: 0.5287\n",
      "Epoch 32/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.5940 - accuracy: 0.5597\n",
      "Epoch 00032: val_accuracy improved from 0.53422 to 0.53645, saving model to ./model./reuters-0032-0.536450.hdf5\n",
      "7185/7185 [==============================] - 3s 437us/sample - loss: 1.5932 - accuracy: 0.5605 - val_loss: 1.7613 - val_accuracy: 0.5364\n",
      "Epoch 33/100\n",
      "7040/7185 [============================>.] - ETA: 0s - loss: 1.5681 - accuracy: 0.5727\n",
      "Epoch 00033: val_accuracy did not improve from 0.53645\n",
      "7185/7185 [==============================] - 3s 438us/sample - loss: 1.5735 - accuracy: 0.5712 - val_loss: 1.8099 - val_accuracy: 0.5153\n",
      "Epoch 34/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.5626 - accuracy: 0.5659\n",
      "Epoch 00034: val_accuracy did not improve from 0.53645\n",
      "7185/7185 [==============================] - 3s 433us/sample - loss: 1.5627 - accuracy: 0.5658 - val_loss: 1.7986 - val_accuracy: 0.5287\n",
      "Epoch 35/100\n",
      "7040/7185 [============================>.] - ETA: 0s - loss: 1.5466 - accuracy: 0.5730\n",
      "Epoch 00035: val_accuracy did not improve from 0.53645\n",
      "7185/7185 [==============================] - 3s 430us/sample - loss: 1.5452 - accuracy: 0.5726 - val_loss: 1.7913 - val_accuracy: 0.5225\n",
      "Epoch 36/100\n",
      "7104/7185 [============================>.] - ETA: 0s - loss: 1.5260 - accuracy: 0.5774\n",
      "Epoch 00036: val_accuracy did not improve from 0.53645\n",
      "7185/7185 [==============================] - 3s 432us/sample - loss: 1.5222 - accuracy: 0.5784 - val_loss: 1.8009 - val_accuracy: 0.5303\n",
      "Epoch 37/100\n",
      "7104/7185 [============================>.] - ETA: 0s - loss: 1.5071 - accuracy: 0.5847\n",
      "Epoch 00037: val_accuracy did not improve from 0.53645\n",
      "7185/7185 [==============================] - 3s 433us/sample - loss: 1.5084 - accuracy: 0.5840 - val_loss: 1.8102 - val_accuracy: 0.5337\n",
      "Epoch 38/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.4987 - accuracy: 0.5869\n",
      "Epoch 00038: val_accuracy did not improve from 0.53645\n",
      "7185/7185 [==============================] - 3s 433us/sample - loss: 1.4985 - accuracy: 0.5868 - val_loss: 1.7914 - val_accuracy: 0.5314\n",
      "Epoch 39/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.4797 - accuracy: 0.5908\n",
      "Epoch 00039: val_accuracy improved from 0.53645 to 0.54257, saving model to ./model./reuters-0039-0.542571.hdf5\n",
      "7185/7185 [==============================] - 3s 436us/sample - loss: 1.4821 - accuracy: 0.5903 - val_loss: 1.8034 - val_accuracy: 0.5426\n",
      "Epoch 40/100\n",
      "7104/7185 [============================>.] - ETA: 0s - loss: 1.4662 - accuracy: 0.5921\n",
      "Epoch 00040: val_accuracy did not improve from 0.54257\n",
      "7185/7185 [==============================] - 3s 434us/sample - loss: 1.4671 - accuracy: 0.5916 - val_loss: 1.8156 - val_accuracy: 0.5314\n",
      "Epoch 41/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.4428 - accuracy: 0.5973\n",
      "Epoch 00041: val_accuracy improved from 0.54257 to 0.54368, saving model to ./model./reuters-0041-0.543684.hdf5\n",
      "7185/7185 [==============================] - 3s 437us/sample - loss: 1.4414 - accuracy: 0.5978 - val_loss: 1.7997 - val_accuracy: 0.5437\n",
      "Epoch 42/100\n",
      "7104/7185 [============================>.] - ETA: 0s - loss: 1.4222 - accuracy: 0.6028\n",
      "Epoch 00042: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 436us/sample - loss: 1.4183 - accuracy: 0.6040 - val_loss: 1.8679 - val_accuracy: 0.5376\n",
      "Epoch 43/100\n",
      "7104/7185 [============================>.] - ETA: 0s - loss: 1.3987 - accuracy: 0.6047\n",
      "Epoch 00043: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 432us/sample - loss: 1.4011 - accuracy: 0.6042 - val_loss: 1.8219 - val_accuracy: 0.5381\n",
      "Epoch 44/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.3844 - accuracy: 0.6114\n",
      "Epoch 00044: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 437us/sample - loss: 1.3845 - accuracy: 0.6109 - val_loss: 1.8371 - val_accuracy: 0.5364\n",
      "Epoch 45/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 1.3698 - accuracy: 0.6168\n",
      "Epoch 00045: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 434us/sample - loss: 1.3693 - accuracy: 0.6168 - val_loss: 1.8686 - val_accuracy: 0.5326\n",
      "Epoch 46/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 1.3545 - accuracy: 0.6210\n",
      "Epoch 00046: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 436us/sample - loss: 1.3544 - accuracy: 0.6209 - val_loss: 1.8476 - val_accuracy: 0.5248\n",
      "Epoch 47/100\n",
      "7040/7185 [============================>.] - ETA: 0s - loss: 1.3315 - accuracy: 0.6286\n",
      "Epoch 00047: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 437us/sample - loss: 1.3291 - accuracy: 0.6299 - val_loss: 1.8534 - val_accuracy: 0.5326\n",
      "Epoch 48/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.3156 - accuracy: 0.6318\n",
      "Epoch 00048: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 438us/sample - loss: 1.3133 - accuracy: 0.6326 - val_loss: 1.8810 - val_accuracy: 0.5281\n",
      "Epoch 49/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.2851 - accuracy: 0.6369\n",
      "Epoch 00049: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 433us/sample - loss: 1.2846 - accuracy: 0.6363 - val_loss: 1.9219 - val_accuracy: 0.5242\n",
      "Epoch 50/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.2702 - accuracy: 0.6393\n",
      "Epoch 00050: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 433us/sample - loss: 1.2666 - accuracy: 0.6399 - val_loss: 1.9306 - val_accuracy: 0.5381\n",
      "Epoch 51/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.2529 - accuracy: 0.6459\n",
      "Epoch 00051: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 436us/sample - loss: 1.2536 - accuracy: 0.6454 - val_loss: 1.9477 - val_accuracy: 0.5303\n",
      "Epoch 52/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 1.2436 - accuracy: 0.6518\n",
      "Epoch 00052: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 436us/sample - loss: 1.2443 - accuracy: 0.6516 - val_loss: 1.9165 - val_accuracy: 0.5387\n",
      "Epoch 53/100\n",
      "7104/7185 [============================>.] - ETA: 0s - loss: 1.2042 - accuracy: 0.6595\n",
      "Epoch 00053: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 435us/sample - loss: 1.2052 - accuracy: 0.6586 - val_loss: 1.9611 - val_accuracy: 0.5326\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.1703 - accuracy: 0.6671\n",
      "Epoch 00054: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 433us/sample - loss: 1.1738 - accuracy: 0.6665 - val_loss: 1.9530 - val_accuracy: 0.5420\n",
      "Epoch 55/100\n",
      "7104/7185 [============================>.] - ETA: 0s - loss: 1.1793 - accuracy: 0.6627\n",
      "Epoch 00055: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 433us/sample - loss: 1.1808 - accuracy: 0.6622 - val_loss: 2.0218 - val_accuracy: 0.5170\n",
      "Epoch 56/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.1585 - accuracy: 0.6646\n",
      "Epoch 00056: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 434us/sample - loss: 1.1584 - accuracy: 0.6644 - val_loss: 2.0145 - val_accuracy: 0.5159\n",
      "Epoch 57/100\n",
      "7040/7185 [============================>.] - ETA: 0s - loss: 1.1391 - accuracy: 0.6744\n",
      "Epoch 00057: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 434us/sample - loss: 1.1416 - accuracy: 0.6739 - val_loss: 2.0413 - val_accuracy: 0.5270\n",
      "Epoch 58/100\n",
      "7104/7185 [============================>.] - ETA: 0s - loss: 1.1064 - accuracy: 0.6837\n",
      "Epoch 00058: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 432us/sample - loss: 1.1081 - accuracy: 0.6836 - val_loss: 2.0696 - val_accuracy: 0.5253\n",
      "Epoch 59/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 1.1033 - accuracy: 0.6825\n",
      "Epoch 00059: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 431us/sample - loss: 1.1056 - accuracy: 0.6818 - val_loss: 2.1074 - val_accuracy: 0.5186\n",
      "Epoch 60/100\n",
      "7040/7185 [============================>.] - ETA: 0s - loss: 1.1019 - accuracy: 0.6810\n",
      "Epoch 00060: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 434us/sample - loss: 1.1014 - accuracy: 0.6820 - val_loss: 2.0866 - val_accuracy: 0.5359\n",
      "Epoch 61/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 1.0619 - accuracy: 0.6941\n",
      "Epoch 00061: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 431us/sample - loss: 1.0611 - accuracy: 0.6944 - val_loss: 2.0617 - val_accuracy: 0.5220\n"
     ]
    }
   ],
   "source": [
    "h3 = model3.fit(X_train_seq_reshape,y_train,\n",
    "               validation_split = 0.2,\n",
    "               epochs =100,\n",
    "               callbacks = [mc, es])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54787aa",
   "metadata": {},
   "source": [
    "##### 모델링 4: 워드임배딩 레이어 추가\n",
    "- 단순하게 단어를 구분하는 원핫인코딩, 단어의 빈도를 사용하는 수치화방법은 \n",
    "- 자연어를 해석하기에 무리가 있음\n",
    "\n",
    "- 단어 사이의 관계를 밀집된 표현으로 만들어줄 수 있는 워드 임베딩 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3553270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 워드임베딩\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a97bd5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SM2127\\anaconda3\\envs\\deep01\\lib\\site-packages\\tensorflow_core\\python\\keras\\datasets\\reuters.py:113: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\SM2127\\anaconda3\\envs\\deep01\\lib\\site-packages\\tensorflow_core\\python\\keras\\datasets\\reuters.py:114: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "# 사용하는 단어의 수를 제한하자(빈도가 낮은 단어들을 삭제)\n",
    "(X_train,y_train),(X_test,y_test) = reuters.load_data(num_words = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dfd867f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 길이가 긴건 자르고 짧은건 늘려준다\n",
    "X_train_seq = sequence.pad_sequences(X_train,maxlen = 120)\n",
    "X_test_seq = sequence.pad_sequences(X_test,maxlen = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a1c9357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 603\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 50)          50000     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, None, 128)         91648     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, None, 64)          49408     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 231,230\n",
      "Trainable params: 231,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "\n",
    "# return_sequeces = True : 뉴런마다 똑같은 데이터가 들어갈 수 있게 한다\n",
    "# 임베딩 레이어는 항상 첫번째 층에 사용\n",
    "#(사용하는 단어의 수 , 각 단어가 표현될 숫자의 갯수 = feature수)\n",
    "model4.add(Embedding(1000,50))\n",
    "model4.add(LSTM(128,return_sequences = True)) #120개길이, 단어하나\n",
    "model4.add(LSTM(64, return_sequences = True))\n",
    "model4.add(LSTM(64))\n",
    "\n",
    "# 기본적으로 모든 층이 다수입력 단일출력\n",
    "# RNN을 여러 층을 쌓으려고 생각한다면 각 층을 다수입력 다수출력으로 바꿔야함\n",
    "# 분류기로 넘겨줄때 특성값을 넘겨줘야 하기 때문에\n",
    "# return_sequences 가 필요없음\n",
    "\n",
    "model4.add(Dense(64,activation = 'relu'))\n",
    "model4.add(Dense(46,activation = 'softmax')) # 0~45클래스중 하나의 값 출력\n",
    "                                            #46개의 결과값을 출력하고 각각\n",
    "                                            #몇퍼센트 해당하는지 알아보기위해서\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f65cdac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.compile(loss = 'sparse_categorical_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "88b2683b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 604\n",
      "Train on 7185 samples, validate on 1797 samples\n",
      "Epoch 1/100\n",
      "7104/7185 [============================>.] - ETA: 0s - loss: 2.5065 - accuracy: 0.3487\n",
      "Epoch 00001: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 7s 1ms/sample - loss: 2.5027 - accuracy: 0.3491 - val_loss: 2.4259 - val_accuracy: 0.3450\n",
      "Epoch 2/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 2.4159 - accuracy: 0.3538\n",
      "Epoch 00002: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 4s 490us/sample - loss: 2.4157 - accuracy: 0.3534 - val_loss: 2.4090 - val_accuracy: 0.3450\n",
      "Epoch 3/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 2.4142 - accuracy: 0.3544\n",
      "Epoch 00003: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 487us/sample - loss: 2.4152 - accuracy: 0.3534 - val_loss: 2.4126 - val_accuracy: 0.3450\n",
      "Epoch 4/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 2.4148 - accuracy: 0.3532\n",
      "Epoch 00004: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 486us/sample - loss: 2.4147 - accuracy: 0.3534 - val_loss: 2.4127 - val_accuracy: 0.3450\n",
      "Epoch 5/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 2.4137 - accuracy: 0.3531\n",
      "Epoch 00005: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 483us/sample - loss: 2.4128 - accuracy: 0.3534 - val_loss: 2.4072 - val_accuracy: 0.3450\n",
      "Epoch 6/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 2.4102 - accuracy: 0.3539\n",
      "Epoch 00006: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 4s 491us/sample - loss: 2.4108 - accuracy: 0.3534 - val_loss: 2.4146 - val_accuracy: 0.3450\n",
      "Epoch 7/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 2.4115 - accuracy: 0.3531\n",
      "Epoch 00007: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 4s 512us/sample - loss: 2.4122 - accuracy: 0.3534 - val_loss: 2.4181 - val_accuracy: 0.3450\n",
      "Epoch 8/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 2.4123 - accuracy: 0.3541\n",
      "Epoch 00008: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 4s 487us/sample - loss: 2.4144 - accuracy: 0.3534 - val_loss: 2.4061 - val_accuracy: 0.3450\n",
      "Epoch 9/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 2.4105 - accuracy: 0.3525\n",
      "Epoch 00009: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 4s 489us/sample - loss: 2.4111 - accuracy: 0.3534 - val_loss: 2.4140 - val_accuracy: 0.3450\n",
      "Epoch 10/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 2.1643 - accuracy: 0.4512\n",
      "Epoch 00010: val_accuracy did not improve from 0.54368\n",
      "7185/7185 [==============================] - 3s 485us/sample - loss: 2.1664 - accuracy: 0.4504 - val_loss: 2.0348 - val_accuracy: 0.4953\n",
      "Epoch 11/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.9907 - accuracy: 0.5003\n",
      "Epoch 00011: val_accuracy improved from 0.54368 to 0.54925, saving model to ./model./reuters-0011-0.549249.hdf5\n",
      "7185/7185 [==============================] - 4s 487us/sample - loss: 1.9833 - accuracy: 0.5019 - val_loss: 1.8001 - val_accuracy: 0.5492\n",
      "Epoch 12/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.9052 - accuracy: 0.5167\n",
      "Epoch 00012: val_accuracy did not improve from 0.54925\n",
      "7185/7185 [==============================] - 3s 486us/sample - loss: 1.9077 - accuracy: 0.5158 - val_loss: 1.9837 - val_accuracy: 0.5036\n",
      "Epoch 13/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.7630 - accuracy: 0.5474\n",
      "Epoch 00013: val_accuracy improved from 0.54925 to 0.55760, saving model to ./model./reuters-0013-0.557596.hdf5\n",
      "7185/7185 [==============================] - 3s 487us/sample - loss: 1.7621 - accuracy: 0.5474 - val_loss: 1.7038 - val_accuracy: 0.5576\n",
      "Epoch 14/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.6586 - accuracy: 0.5679\n",
      "Epoch 00014: val_accuracy improved from 0.55760 to 0.56594, saving model to ./model./reuters-0014-0.565943.hdf5\n",
      "7185/7185 [==============================] - 4s 489us/sample - loss: 1.6545 - accuracy: 0.5690 - val_loss: 1.6719 - val_accuracy: 0.5659\n",
      "Epoch 15/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.6160 - accuracy: 0.5768\n",
      "Epoch 00015: val_accuracy improved from 0.56594 to 0.57819, saving model to ./model./reuters-0015-0.578186.hdf5\n",
      "7185/7185 [==============================] - 4s 488us/sample - loss: 1.6137 - accuracy: 0.5775 - val_loss: 1.6180 - val_accuracy: 0.5782\n",
      "Epoch 16/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.5807 - accuracy: 0.5778\n",
      "Epoch 00016: val_accuracy improved from 0.57819 to 0.58709, saving model to ./model./reuters-0016-0.587090.hdf5\n",
      "7185/7185 [==============================] - 4s 487us/sample - loss: 1.5788 - accuracy: 0.5781 - val_loss: 1.5922 - val_accuracy: 0.5871\n",
      "Epoch 17/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.5416 - accuracy: 0.5908\n",
      "Epoch 00017: val_accuracy did not improve from 0.58709\n",
      "7185/7185 [==============================] - 3s 484us/sample - loss: 1.5389 - accuracy: 0.5923 - val_loss: 1.5777 - val_accuracy: 0.5838\n",
      "Epoch 18/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.5200 - accuracy: 0.5942\n",
      "Epoch 00018: val_accuracy improved from 0.58709 to 0.60211, saving model to ./model./reuters-0018-0.602115.hdf5\n",
      "7185/7185 [==============================] - 3s 486us/sample - loss: 1.5205 - accuracy: 0.5942 - val_loss: 1.5785 - val_accuracy: 0.6021\n",
      "Epoch 19/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.4822 - accuracy: 0.6110\n",
      "Epoch 00019: val_accuracy improved from 0.60211 to 0.60434, saving model to ./model./reuters-0019-0.604341.hdf5\n",
      "7185/7185 [==============================] - 3s 485us/sample - loss: 1.4856 - accuracy: 0.6109 - val_loss: 1.5514 - val_accuracy: 0.6043\n",
      "Epoch 20/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.4570 - accuracy: 0.6227\n",
      "Epoch 00020: val_accuracy improved from 0.60434 to 0.62104, saving model to ./model./reuters-0020-0.621035.hdf5\n",
      "7185/7185 [==============================] - 4s 491us/sample - loss: 1.4577 - accuracy: 0.6228 - val_loss: 1.6185 - val_accuracy: 0.6210\n",
      "Epoch 21/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.4565 - accuracy: 0.6340\n",
      "Epoch 00021: val_accuracy improved from 0.62104 to 0.63996, saving model to ./model./reuters-0021-0.639955.hdf5\n",
      "7185/7185 [==============================] - 4s 489us/sample - loss: 1.4554 - accuracy: 0.6344 - val_loss: 1.4794 - val_accuracy: 0.6400\n",
      "Epoch 22/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.3841 - accuracy: 0.6537\n",
      "Epoch 00022: val_accuracy improved from 0.63996 to 0.64441, saving model to ./model./reuters-0022-0.644407.hdf5\n",
      "7185/7185 [==============================] - 4s 490us/sample - loss: 1.3885 - accuracy: 0.6522 - val_loss: 1.4627 - val_accuracy: 0.6444\n",
      "Epoch 23/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.3490 - accuracy: 0.6643\n",
      "Epoch 00023: val_accuracy improved from 0.64441 to 0.65442, saving model to ./model./reuters-0023-0.654424.hdf5\n",
      "7185/7185 [==============================] - 4s 499us/sample - loss: 1.3482 - accuracy: 0.6647 - val_loss: 1.4407 - val_accuracy: 0.6544\n",
      "Epoch 24/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.3180 - accuracy: 0.6725\n",
      "Epoch 00024: val_accuracy improved from 0.65442 to 0.66221, saving model to ./model./reuters-0024-0.662215.hdf5\n",
      "7185/7185 [==============================] - 4s 496us/sample - loss: 1.3137 - accuracy: 0.6736 - val_loss: 1.4080 - val_accuracy: 0.6622\n",
      "Epoch 25/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.2776 - accuracy: 0.6797\n",
      "Epoch 00025: val_accuracy improved from 0.66221 to 0.67112, saving model to ./model./reuters-0025-0.671119.hdf5\n",
      "7185/7185 [==============================] - 4s 500us/sample - loss: 1.2764 - accuracy: 0.6799 - val_loss: 1.3751 - val_accuracy: 0.6711\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.2464 - accuracy: 0.6875\n",
      "Epoch 00026: val_accuracy did not improve from 0.67112\n",
      "7185/7185 [==============================] - 3s 483us/sample - loss: 1.2462 - accuracy: 0.6878 - val_loss: 1.4197 - val_accuracy: 0.6617\n",
      "Epoch 27/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.2131 - accuracy: 0.6926\n",
      "Epoch 00027: val_accuracy did not improve from 0.67112\n",
      "7185/7185 [==============================] - 4s 491us/sample - loss: 1.2122 - accuracy: 0.6928 - val_loss: 1.3579 - val_accuracy: 0.6672\n",
      "Epoch 28/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.1747 - accuracy: 0.7025\n",
      "Epoch 00028: val_accuracy did not improve from 0.67112\n",
      "7185/7185 [==============================] - 3s 485us/sample - loss: 1.1749 - accuracy: 0.7029 - val_loss: 1.3446 - val_accuracy: 0.6672\n",
      "Epoch 29/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.1464 - accuracy: 0.7053\n",
      "Epoch 00029: val_accuracy improved from 0.67112 to 0.67390, saving model to ./model./reuters-0029-0.673901.hdf5\n",
      "7185/7185 [==============================] - 3s 479us/sample - loss: 1.1489 - accuracy: 0.7054 - val_loss: 1.3481 - val_accuracy: 0.6739\n",
      "Epoch 30/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.1118 - accuracy: 0.7144\n",
      "Epoch 00030: val_accuracy improved from 0.67390 to 0.68058, saving model to ./model./reuters-0030-0.680579.hdf5\n",
      "7185/7185 [==============================] - 3s 478us/sample - loss: 1.1171 - accuracy: 0.7132 - val_loss: 1.3450 - val_accuracy: 0.6806\n",
      "Epoch 31/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.0980 - accuracy: 0.7163\n",
      "Epoch 00031: val_accuracy did not improve from 0.68058\n",
      "7185/7185 [==============================] - 3s 479us/sample - loss: 1.1007 - accuracy: 0.7157 - val_loss: 1.3301 - val_accuracy: 0.6795\n",
      "Epoch 32/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.0659 - accuracy: 0.7220\n",
      "Epoch 00032: val_accuracy did not improve from 0.68058\n",
      "7185/7185 [==============================] - 3s 476us/sample - loss: 1.0649 - accuracy: 0.7225 - val_loss: 1.3568 - val_accuracy: 0.6761\n",
      "Epoch 33/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.0325 - accuracy: 0.7346\n",
      "Epoch 00033: val_accuracy did not improve from 0.68058\n",
      "7185/7185 [==============================] - 3s 473us/sample - loss: 1.0306 - accuracy: 0.7351 - val_loss: 1.3566 - val_accuracy: 0.6700\n",
      "Epoch 34/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 1.0033 - accuracy: 0.7394\n",
      "Epoch 00034: val_accuracy improved from 0.68058 to 0.69449, saving model to ./model./reuters-0034-0.694491.hdf5\n",
      "7185/7185 [==============================] - 3s 477us/sample - loss: 1.0012 - accuracy: 0.7396 - val_loss: 1.3315 - val_accuracy: 0.6945\n",
      "Epoch 35/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.9755 - accuracy: 0.7453\n",
      "Epoch 00035: val_accuracy did not improve from 0.69449\n",
      "7185/7185 [==============================] - 3s 478us/sample - loss: 0.9802 - accuracy: 0.7443 - val_loss: 1.3250 - val_accuracy: 0.6889\n",
      "Epoch 36/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.9560 - accuracy: 0.7547\n",
      "Epoch 00036: val_accuracy did not improve from 0.69449\n",
      "7185/7185 [==============================] - 3s 478us/sample - loss: 0.9540 - accuracy: 0.7548 - val_loss: 1.3540 - val_accuracy: 0.6867\n",
      "Epoch 37/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 0.9448 - accuracy: 0.7542\n",
      "Epoch 00037: val_accuracy did not improve from 0.69449\n",
      "7185/7185 [==============================] - 4s 518us/sample - loss: 0.9450 - accuracy: 0.7541 - val_loss: 1.3375 - val_accuracy: 0.6923\n",
      "Epoch 38/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.9106 - accuracy: 0.7634\n",
      "Epoch 00038: val_accuracy did not improve from 0.69449\n",
      "7185/7185 [==============================] - 4s 496us/sample - loss: 0.9089 - accuracy: 0.7637 - val_loss: 1.3517 - val_accuracy: 0.6917\n",
      "Epoch 39/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.8765 - accuracy: 0.7786\n",
      "Epoch 00039: val_accuracy did not improve from 0.69449\n",
      "7185/7185 [==============================] - 4s 488us/sample - loss: 0.8757 - accuracy: 0.7780 - val_loss: 1.3566 - val_accuracy: 0.6900\n",
      "Epoch 40/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.8536 - accuracy: 0.7783\n",
      "Epoch 00040: val_accuracy did not improve from 0.69449\n",
      "7185/7185 [==============================] - 4s 488us/sample - loss: 0.8516 - accuracy: 0.7793 - val_loss: 1.3720 - val_accuracy: 0.6817\n",
      "Epoch 41/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.8412 - accuracy: 0.7808\n",
      "Epoch 00041: val_accuracy improved from 0.69449 to 0.69894, saving model to ./model./reuters-0041-0.698943.hdf5\n",
      "7185/7185 [==============================] - 4s 493us/sample - loss: 0.8421 - accuracy: 0.7807 - val_loss: 1.3424 - val_accuracy: 0.6989\n",
      "Epoch 42/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 0.8090 - accuracy: 0.7927\n",
      "Epoch 00042: val_accuracy did not improve from 0.69894\n",
      "7185/7185 [==============================] - 4s 514us/sample - loss: 0.8074 - accuracy: 0.7932 - val_loss: 1.3700 - val_accuracy: 0.6962\n",
      "Epoch 43/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.7867 - accuracy: 0.7955\n",
      "Epoch 00043: val_accuracy did not improve from 0.69894\n",
      "7185/7185 [==============================] - 4s 512us/sample - loss: 0.7905 - accuracy: 0.7947 - val_loss: 1.3490 - val_accuracy: 0.6967\n",
      "Epoch 44/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.7631 - accuracy: 0.8054\n",
      "Epoch 00044: val_accuracy did not improve from 0.69894\n",
      "7185/7185 [==============================] - 4s 538us/sample - loss: 0.7641 - accuracy: 0.8051 - val_loss: 1.3920 - val_accuracy: 0.6967\n",
      "Epoch 45/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 0.7452 - accuracy: 0.8071\n",
      "Epoch 00045: val_accuracy did not improve from 0.69894\n",
      "7185/7185 [==============================] - 4s 532us/sample - loss: 0.7447 - accuracy: 0.8071 - val_loss: 1.4221 - val_accuracy: 0.6989\n",
      "Epoch 46/100\n",
      "7168/7185 [============================>.] - ETA: 0s - loss: 0.7241 - accuracy: 0.8146\n",
      "Epoch 00046: val_accuracy did not improve from 0.69894\n",
      "7185/7185 [==============================] - 4s 517us/sample - loss: 0.7252 - accuracy: 0.8145 - val_loss: 1.4661 - val_accuracy: 0.6912\n",
      "Epoch 47/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.7205 - accuracy: 0.8166\n",
      "Epoch 00047: val_accuracy did not improve from 0.69894\n",
      "7185/7185 [==============================] - 3s 477us/sample - loss: 0.7221 - accuracy: 0.8156 - val_loss: 1.3788 - val_accuracy: 0.6939\n",
      "Epoch 48/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.6867 - accuracy: 0.8247\n",
      "Epoch 00048: val_accuracy improved from 0.69894 to 0.70284, saving model to ./model./reuters-0048-0.702838.hdf5\n",
      "7185/7185 [==============================] - 3s 481us/sample - loss: 0.6870 - accuracy: 0.8246 - val_loss: 1.3941 - val_accuracy: 0.7028\n",
      "Epoch 49/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.6746 - accuracy: 0.8257\n",
      "Epoch 00049: val_accuracy did not improve from 0.70284\n",
      "7185/7185 [==============================] - 3s 479us/sample - loss: 0.6775 - accuracy: 0.8248 - val_loss: 1.4123 - val_accuracy: 0.6889\n",
      "Epoch 50/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.6698 - accuracy: 0.8232\n",
      "Epoch 00050: val_accuracy did not improve from 0.70284\n",
      "7185/7185 [==============================] - 3s 476us/sample - loss: 0.6693 - accuracy: 0.8235 - val_loss: 1.4142 - val_accuracy: 0.6973\n",
      "Epoch 51/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.6307 - accuracy: 0.8402\n",
      "Epoch 00051: val_accuracy did not improve from 0.70284\n",
      "7185/7185 [==============================] - 3s 477us/sample - loss: 0.6310 - accuracy: 0.8401 - val_loss: 1.5003 - val_accuracy: 0.6989\n",
      "Epoch 52/100\n",
      "7136/7185 [============================>.] - ETA: 0s - loss: 0.6101 - accuracy: 0.8412\n",
      "Epoch 00052: val_accuracy improved from 0.70284 to 0.70506, saving model to ./model./reuters-0052-0.705064.hdf5\n",
      "7185/7185 [==============================] - 4s 502us/sample - loss: 0.6099 - accuracy: 0.8413 - val_loss: 1.4833 - val_accuracy: 0.7051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.5914 - accuracy: 0.8498\n",
      "Epoch 00053: val_accuracy improved from 0.70506 to 0.70673, saving model to ./model./reuters-0053-0.706733.hdf5\n",
      "7185/7185 [==============================] - 3s 481us/sample - loss: 0.5913 - accuracy: 0.8501 - val_loss: 1.4918 - val_accuracy: 0.7067\n",
      "Epoch 54/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.6160 - accuracy: 0.8447\n",
      "Epoch 00054: val_accuracy did not improve from 0.70673\n",
      "7185/7185 [==============================] - 3s 478us/sample - loss: 0.6151 - accuracy: 0.8450 - val_loss: 1.4977 - val_accuracy: 0.6962\n",
      "Epoch 55/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.5888 - accuracy: 0.8510\n",
      "Epoch 00055: val_accuracy did not improve from 0.70673\n",
      "7185/7185 [==============================] - 3s 478us/sample - loss: 0.5889 - accuracy: 0.8507 - val_loss: 1.4749 - val_accuracy: 0.6989\n",
      "Epoch 56/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.5378 - accuracy: 0.8640\n",
      "Epoch 00056: val_accuracy did not improve from 0.70673\n",
      "7185/7185 [==============================] - 3s 479us/sample - loss: 0.5416 - accuracy: 0.8628 - val_loss: 1.5405 - val_accuracy: 0.6967\n",
      "Epoch 57/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.5272 - accuracy: 0.8652\n",
      "Epoch 00057: val_accuracy did not improve from 0.70673\n",
      "7185/7185 [==============================] - 3s 474us/sample - loss: 0.5293 - accuracy: 0.8642 - val_loss: 1.5525 - val_accuracy: 0.7001\n",
      "Epoch 58/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.5100 - accuracy: 0.8688\n",
      "Epoch 00058: val_accuracy did not improve from 0.70673\n",
      "7185/7185 [==============================] - 3s 477us/sample - loss: 0.5103 - accuracy: 0.8688 - val_loss: 1.5846 - val_accuracy: 0.7001\n",
      "Epoch 59/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.5031 - accuracy: 0.8732\n",
      "Epoch 00059: val_accuracy did not improve from 0.70673\n",
      "7185/7185 [==============================] - 3s 479us/sample - loss: 0.5027 - accuracy: 0.8731 - val_loss: 1.6134 - val_accuracy: 0.7067\n",
      "Epoch 60/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.4980 - accuracy: 0.8736\n",
      "Epoch 00060: val_accuracy did not improve from 0.70673\n",
      "7185/7185 [==============================] - 3s 477us/sample - loss: 0.4963 - accuracy: 0.8738 - val_loss: 1.6389 - val_accuracy: 0.7001\n",
      "Epoch 61/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.4734 - accuracy: 0.8792\n",
      "Epoch 00061: val_accuracy did not improve from 0.70673\n",
      "7185/7185 [==============================] - 3s 476us/sample - loss: 0.4751 - accuracy: 0.8791 - val_loss: 1.6556 - val_accuracy: 0.7028\n",
      "Epoch 62/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.4611 - accuracy: 0.8819\n",
      "Epoch 00062: val_accuracy did not improve from 0.70673\n",
      "7185/7185 [==============================] - 3s 482us/sample - loss: 0.4620 - accuracy: 0.8816 - val_loss: 1.6620 - val_accuracy: 0.6950\n",
      "Epoch 63/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.4459 - accuracy: 0.8889\n",
      "Epoch 00063: val_accuracy did not improve from 0.70673\n",
      "7185/7185 [==============================] - 3s 486us/sample - loss: 0.4458 - accuracy: 0.8889 - val_loss: 1.7209 - val_accuracy: 0.7006\n",
      "Epoch 64/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.4346 - accuracy: 0.8882\n",
      "Epoch 00064: val_accuracy did not improve from 0.70673\n",
      "7185/7185 [==============================] - 3s 483us/sample - loss: 0.4333 - accuracy: 0.8884 - val_loss: 1.7459 - val_accuracy: 0.6984\n",
      "Epoch 65/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.4269 - accuracy: 0.8935\n",
      "Epoch 00065: val_accuracy did not improve from 0.70673\n",
      "7185/7185 [==============================] - 3s 486us/sample - loss: 0.4289 - accuracy: 0.8931 - val_loss: 1.7581 - val_accuracy: 0.6995\n",
      "Epoch 66/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.4261 - accuracy: 0.8927\n",
      "Epoch 00066: val_accuracy did not improve from 0.70673\n",
      "7185/7185 [==============================] - 4s 488us/sample - loss: 0.4297 - accuracy: 0.8917 - val_loss: 1.7475 - val_accuracy: 0.7045\n",
      "Epoch 67/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.4242 - accuracy: 0.8923\n",
      "Epoch 00067: val_accuracy did not improve from 0.70673\n",
      "7185/7185 [==============================] - 3s 481us/sample - loss: 0.4232 - accuracy: 0.8924 - val_loss: 1.7478 - val_accuracy: 0.6950\n",
      "Epoch 68/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.4112 - accuracy: 0.8944\n",
      "Epoch 00068: val_accuracy did not improve from 0.70673\n",
      "7185/7185 [==============================] - 3s 482us/sample - loss: 0.4122 - accuracy: 0.8945 - val_loss: 1.7955 - val_accuracy: 0.6995\n",
      "Epoch 69/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.3801 - accuracy: 0.9060\n",
      "Epoch 00069: val_accuracy did not improve from 0.70673\n",
      "7185/7185 [==============================] - 3s 481us/sample - loss: 0.3794 - accuracy: 0.9058 - val_loss: 1.8589 - val_accuracy: 0.6967\n",
      "Epoch 70/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.3719 - accuracy: 0.9048\n",
      "Epoch 00070: val_accuracy did not improve from 0.70673\n",
      "7185/7185 [==============================] - 3s 483us/sample - loss: 0.3742 - accuracy: 0.9041 - val_loss: 1.8538 - val_accuracy: 0.6978\n",
      "Epoch 71/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.3645 - accuracy: 0.9037\n",
      "Epoch 00071: val_accuracy did not improve from 0.70673\n",
      "7185/7185 [==============================] - 3s 481us/sample - loss: 0.3640 - accuracy: 0.9040 - val_loss: 1.9258 - val_accuracy: 0.6861\n",
      "Epoch 72/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.3517 - accuracy: 0.9118\n",
      "Epoch 00072: val_accuracy did not improve from 0.70673\n",
      "7185/7185 [==============================] - 3s 480us/sample - loss: 0.3534 - accuracy: 0.9112 - val_loss: 1.9632 - val_accuracy: 0.6873\n",
      "Epoch 73/100\n",
      "7072/7185 [============================>.] - ETA: 0s - loss: 0.3752 - accuracy: 0.9005\n",
      "Epoch 00073: val_accuracy did not improve from 0.70673\n",
      "7185/7185 [==============================] - 3s 483us/sample - loss: 0.3775 - accuracy: 0.9008 - val_loss: 1.9007 - val_accuracy: 0.6989\n"
     ]
    }
   ],
   "source": [
    "h4 = model4.fit(X_train_seq,y_train, # 임베딩이 자료구조를 변화하기 때문에 \n",
    "                #reshape를 할 필요가 없음\n",
    "               validation_split = 0.2,\n",
    "               epochs =100,\n",
    "               callbacks = [mc, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25825a56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
